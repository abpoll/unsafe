{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07d781b0",
   "metadata": {},
   "source": [
    "# Purpose of this notebook\n",
    "\n",
    "This notebook serves to demonstrate the following workflow to help introduce new users to the UNSAFE framework, and to allow them to test that the code does what it says it does. \n",
    "\n",
    "1) Configure the working directory structure and workflow parameters;\n",
    "2) If you want to make use of the downloading functionality in UNSAFE, download and unzip data;\n",
    "3) Subset the full structure inventory to single-family structures and convert the data to a GeoDataFrame;\n",
    "4) Specify a spatial extent of your study area (i.e. county shapefile or a study boundary) and process reference files through clipping (e.g. census tract data);\n",
    "5) Process expert DDFs for use in ensembles;\n",
    "6) Process social vulnerability data by linking it with corresponding reference data (e.g. linking Climate and Economic Justice Screening Tool with census tracts);\n",
    "7) Prepare the National Flood Hazard Layer data for identifying structure location in and outside of the federal floodplain;\n",
    "8) Link structures to all vector spatial data;\n",
    "9) Link structures to inundation data, provided as raster(s);\n",
    "10) Prepare the structure inventory for loss estimation (this base inventory can be used for estimating losses without uncertainty);\n",
    "11) Generate an ensemble of plausible structure realizations based on several parameters users can specify;\n",
    "12) Estimate expected annual losses for each ensemble member for a set of design events. Each ensemble member has a unique draw from the DDF distribution. Users can also estimate expected annual losses without uncertainty in exposure and vulnerability. \n",
    "\n",
    "\n",
    "We also provide code for producing some visualizations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed264f1c-60c1-49bd-af7b-ca42fd88e194",
   "metadata": {},
   "source": [
    "# Configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedcdf6b-f5df-4b72-b285-3313df884f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are options we like setting for working with jupyter notebooks. Totally optional\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836c741b",
   "metadata": {},
   "source": [
    "We load packages (mostly from UNSAFE) to help us get through the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cbc7fd-af6d-4800-86c4-4ea008bf85d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import os\n",
    "from os.path import join\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "from yaml.loader import SafeLoader\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "import unsafe.download as undown\n",
    "import unsafe.files as unfile\n",
    "import unsafe.unzip as ununzip\n",
    "import unsafe.exp as unexp\n",
    "import unsafe.ddfs as unddf\n",
    "import unsafe.ensemble as unens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51bd764",
   "metadata": {},
   "source": [
    "Next, we configure spatial units like county fips. Right now, the county is the unit of analysis for UNSAFE. The county fips code is used to download data from the national structure inventory API, and for organzing the directory structure. This facilitates distributed county-level analyses (though we have not tested this yet, and there are likely some things to work out before then). We also keep track of the state fips code (the first two digits of the county code) for downloading data and organizing the file directory. The state abbrevation for a county can be helpful (though it's not currently used). We also have the \"Nation\" indicator, since damage functions have been produced in other countries. Keeping track of nation is probably not a feature that will remain in UNSAFE in future versions. In future versions, we plan to have state fips and state abbrevation inferred from the user-supplied FIPS code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58884488-133d-4dd2-9d44-6fba2ac1526a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name the fips, statefips, stateabbr, and nation that\n",
    "# we are using for this analysis\n",
    "# We pass these in as a list even though the framework currently\n",
    "# processes a single county so that it can facilitate that\n",
    "# expansion in the future\n",
    "# TODO - could make sense to define these in the future\n",
    "# in json or other formats instead of as input in code\n",
    "fips_args = {\n",
    "    'FIPS': ['42101'], \n",
    "    'STATEFIPS': ['42'],\n",
    "    'STATEABBR': ['PA'],\n",
    "    'NATION': ['US']\n",
    "}\n",
    "FIPS = fips_args['FIPS'][0]\n",
    "NATION = fips_args['NATION'][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e042cfc",
   "metadata": {},
   "source": [
    "Most of the configuration for UNSAFE is about setting up the file directory structure. Users identify the absolute directory to the root of their project directory and store this in ABS_DIR. This is where we recommend storing a config.yaml file in a `config/` directory. Below, we show all the file directory configurations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad060e58-0489-4bf4-8e24-6728dd15c0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to pass in a config file that sets up\n",
    "# constants and the structure for downlading data\n",
    "# For the directory structure of our case study, \n",
    "# we use the following, but you may\n",
    "# have to specify a different Path\n",
    "ABS_DIR = Path().absolute().parents[0]\n",
    "\n",
    "\n",
    "# We are updating the config filename because for this example\n",
    "# we are using different hazard data. The HAZ_FILEN parameter\n",
    "# will change values. Everything else is the same. \n",
    "CONFIG_FILEP = join(ABS_DIR, 'config', 'config_partial.yaml')\n",
    "# Open the config file and load\n",
    "with open(CONFIG_FILEP) as f:\n",
    "    CONFIG = yaml.load(f, Loader=SafeLoader)\n",
    "\n",
    "# We can also specify the filepath to the\n",
    "# raw data directory\n",
    "FR = join(ABS_DIR, \"data\", \"raw\")\n",
    "\n",
    "# And external - where our hazard data should be\n",
    "FE = join(FR, \"external\")\n",
    "\n",
    "# Set up interim and results directories as well\n",
    "# We already use \"FR\" for raw, we use \"FO\" \n",
    "# because you can also think of results\n",
    "# as output\n",
    "FI = join(ABS_DIR, \"data\", \"interim\")\n",
    "FO = join(ABS_DIR, \"data\", \"results\")\n",
    "\n",
    "# \"Raw\" data directories for exposure, vulnerability (vuln) and\n",
    "# administrative reference files\n",
    "EXP_DIR_R = join(FR, \"exp\")\n",
    "VULN_DIR_R = join(FR, \"vuln\")\n",
    "REF_DIR_R = join(FR, \"ref\")\n",
    "# Haz is for depth grids\n",
    "HAZ_DIR_R = join(FE, \"haz\")\n",
    "# Pol is for NFHL\n",
    "POL_DIR_R = join(FR, \"pol\")\n",
    "\n",
    "# Unzip directory \n",
    "UNZIP_DIR = join(FR, \"unzipped\")\n",
    "\n",
    "# We want to process unzipped data and move it\n",
    "# to the interim directory where we keep\n",
    "# processed data\n",
    "# Get the filepaths for unzipped data\n",
    "# We unzipped the depth grids (haz) and \n",
    "# ddfs (vuln) into the \"external\"/ subdirectory\n",
    "HAZ_DIR_UZ = join(UNZIP_DIR, \"external\", \"haz\")\n",
    "POL_DIR_UZ = join(UNZIP_DIR, \"pol\")\n",
    "REF_DIR_UZ = join(UNZIP_DIR, \"ref\")\n",
    "VULN_DIR_UZ = join(UNZIP_DIR, \"external\", \"vuln\")\n",
    "\n",
    "# \"Interim\" data directories\n",
    "EXP_DIR_I = join(FI, \"exp\")\n",
    "VULN_DIR_I = join(FI, \"vuln\")\n",
    "REF_DIR_I = join(FI, \"ref\")\n",
    "# Haz is for depth grids\n",
    "HAZ_DIR_I = join(FI, \"haz\")\n",
    "# Pol is for NFHL\n",
    "POL_DIR_I = join(FI, \"pol\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265ef389",
   "metadata": {},
   "source": [
    "Next, we provide detailed comments on the other things in the config.yaml file.\n",
    "\n",
    "First we get the files we need downloaded. These are specified in the 'download' key in the config file. We transpose the table because of the way other functions use the DOWNLOAD dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae11502",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOAD = pd.json_normalize(CONFIG['download'], sep='_').T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c3591a",
   "metadata": {},
   "source": [
    "Next, we want 'url wildcards' which will help us take the generic urls from the DOWNLOAD dictionary and fill them in with case-study specific deatils. For example, {FIPS} is a wildcard which will be replaced by the FIPS argument you specified in fips_arg for this case study. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0bef78",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_WILDCARDS = CONFIG['url_wildcards']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68d730f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the file extensions for api endpoints\n",
    "# In our case study, this is only for downloading from the NSI\n",
    "API_EXT = CONFIG['api_ext']\n",
    "\n",
    "# Found the NSI CRS online\n",
    "NSI_CRS = CONFIG['nsi_crs']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b68b04d",
   "metadata": {},
   "source": [
    "We also have a dictionary of reference names which helps us standardize how we refer to census tract, block group, etc. ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d7f4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "REF_NAMES_DICT = CONFIG['ref_names']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c68e72",
   "metadata": {},
   "source": [
    "We also specify a dictonary of reference id names. This converts things like \"GEOID\" to \"tract_id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f936e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "REF_ID_NAMES_DICT = CONFIG['ref_id_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364eaf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficient of variation\n",
    "# for structure values\n",
    "# This is what we scale the structure value by\n",
    "# to get the standard deviation we draw from\n",
    "COEF_VARIATION = CONFIG['coef_var']\n",
    "\n",
    "# First floor elevation dictionary\n",
    "# This maps foundation types to the triangular distributions\n",
    "# for first-floor elevation\n",
    "FFE_DICT = CONFIG['ffe_dict']\n",
    "\n",
    "# Number of states of the world\n",
    "# This is the number of ensemble members\n",
    "N_SOW = CONFIG['sows']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420d958d",
   "metadata": {},
   "source": [
    "The hazard configuration will vary on a case study by case study basis. In this example, we show you how to define a configuration file that corresponds to a set of design flood event model runs. This type of hazard input allows you to estimate expected annual loss, which we demonstrate later in the tutorial. The below configuration can work on any depth grids from the FEMA Flood Risk Database. We use the riverine flooding products from the database for this case study. We are working on making a larger set of config file examples for different kinds of flood model representations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd27976e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get hazard model variables\n",
    "# Get Return Period list\n",
    "RET_PERS = CONFIG['RPs']\n",
    "HAZ_FILEN = CONFIG['haz_filename']\n",
    "# Get CRS for depth grids\n",
    "HAZ_CRS = CONFIG['haz_crs']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1416bf35-5d86-45c8-9869-617e4709f46a",
   "metadata": {},
   "source": [
    "# Download (and unzip) data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e59c4b5",
   "metadata": {},
   "source": [
    "We added download functionality to UNSAFE because it can  help make an analysis more reproducible. Instead of manually downloading hazard, exposure, and vulnerability data from different sources, much of (and sometimes all of) the data needed for a standard risk assessment can be specified using URL or API endpoints. \n",
    "\n",
    "We download data with `download_raw(files, wcard_dict, fr, api_ext)` and then unzip data -- both downloaded and user uploaded -- using `unzip_raw(fr, unzip_dir)`. In this partial data example, some of the data is manually uploaded in the `philadelphia_frd/data/raw/external/` directory. When you download UNSAFE, it comes with `external/haz/dg_clip.zip` and `external/vuln/ddfs.zip`. The first directory corresponds to the hazard data we will use for the case study, which originally comes from the Federal Emergency Managemeng Agency (FEMA) Philadelphia Flood Risk Database. You can manually download the data we used by going to [the FEMA Flood Map Service Center: Search All Products](https://msc.fema.gov/portal/advanceSearch#searchresultsanchor) and searching for \"FRD_02040202_PA_GeoTIFFs\" in the Product ID search box. This data is over 2 GB. You can use this data to follow the `full_data_example.ipynb` notebook. We clipped this raw data to a small spatial extent to allow for the partial example shown here. The second directory, `external/vuln/ddfs.zip`, corresponds to the Depth-Damage Functions, downloaded from [here](https://zenodo.org/records/10027236). This Zenodo repository is well-documented and includes a detailed explanation of where the records are originally from. We manually download these and store them in the data directory for the case study because the URL endpoints for most FEMA Flood Map Service Center products are difficult to identify, and the process for accessing the Zenodo API is somewhat complex. \n",
    "\n",
    "The `files` object (specified in CONFIG['DOWNLOAD']) should be structured in a nested dictionary in the following format: SPATIAL_UNIT -> ENDPOINT_TYPE -> COMPONENT -> FILENAME: ENDPOINT. As an example, to download the structures for a county from the NSI, you would specify `FIPS: api: exp: nsi: \"https://nsi.sec.usace.army.mil/nsiapi/structures?fips={FIPS}\"`. The `wcard_dict` is specified in our configuration step above, and will convert {FIPS} to our county code. \"api\" tells the function that we are downloading data from an api endpoint (we could also specify url), and exp tells the function that this is an exposure dataset and should be organized in the working directory accordingly. Other \"COMPONENT\"s include vuln for vulnerability, haz for hazard, pol for policy (e.g. National Flood Hazard Layer), and ref for reference (e.g. censust tract boundaries). We clarify when vulnerability refers to social or physical vulnerability with an additional nested dictionary, like COMPONENT -> SUBCOMPONENT -> FILENAME: ENDPOINT. \n",
    "\n",
    "Some endpoints need to be manually specified (e.g. the URL for the National Flood Hazard Layer) because we do not know how to uniquely identify the corresponding data (and the FEMA Help Desk did not respond to our emails asking for guidance). We believe it is a more reproducible practice to specify the download links when possible, as opposed to manually downloading files and uploading them to the working directory. However, users do not have to use the download functionality in UNSAFE. Users could treat all data as external to the UNSAFE workflow, and would configure the working environment accordingly. We recommend putting this data in `ABS_DIR/data/raw/external/` with subdirectories for a specific COMPONENT, but you can modify the filepaths defined above if you'd like to put this data somewhere else. \n",
    "\n",
    "`fr` corresponds to the relative filepath to the raw data directory. UNSAFE currently gives users the options to customize the directory structure in the configuration step, but enforces the nested directory structure within that which separates data by SPATIAL_UNIT and COMPONENT. We find that the raw, interim, and results structure provided in this tutorial is helpful for many types of risk assessments. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4688c84",
   "metadata": {},
   "source": [
    "The unsafe.download library provides us with convenient functions for quickly dowloading data.\n",
    "\n",
    "As mentioned before, URL_WILDCARDS has entires like {FIPS} which we are going to replace from the fips_args dictionary we defined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1cd00d-1c06-43ac-ade6-dc949e99d8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "wcard_dict = {x: fips_args[x[1:-1]][0] for x in URL_WILDCARDS}\n",
    "undown.download_raw(DOWNLOAD, wcard_dict,\n",
    "                    FR, API_EXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe77df80",
   "metadata": {},
   "source": [
    "We then call the `unzip_raw()` function that unzips any .zip file in raw/ for us. We carefully structured the downloaded data, and the data we uploaded to raw/external/ to make it easier to move files around from raw/ to interim/ to results/ directories and find them later. This is something you can customize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd95446c-adaa-4252-b94c-4adbe1e75a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "ununzip.unzip_raw(FR, UNZIP_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71388923",
   "metadata": {},
   "source": [
    "Both of the functions we just called do a lot of work under the hood for us, calling helper functions in `unsafe.download` and `unsafe.unzip`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d5b9de-7fea-4474-ae90-47127e366661",
   "metadata": {},
   "source": [
    "# Prepare data for ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc920086",
   "metadata": {},
   "source": [
    "This section comprises steps 3 through 9 mentioned at the top of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2167d3cf-adf4-4b5e-80e7-2db7873641cb",
   "metadata": {},
   "source": [
    "## Exposure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d61d1c",
   "metadata": {},
   "source": [
    "Now that we have downloaded the NSI records for the county, we are going to do some preprocessing for generating our ensemble.\n",
    "\n",
    "First, we will call `get_nsi_geo()` to use the spatial coordinates from the NSI and prepare a GeoDataFrame. We need the data in this format for linking other attributes needed for loss estimation with the NSI, like flood depths. \n",
    "\n",
    "Next, we subset the records from the NSI using SQL-like querying on our pandas DataFrame. \n",
    "\n",
    "These steps give us a GeoDataFrame of single family residences for our ensemble. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097af6db",
   "metadata": {},
   "source": [
    "For this case study we are focusing on properties with 'RES1' values in the 'occtype' column of the raw NSI data. We willf ocus on properties with at most 2 stories because we have DDFs for 1 and 2 story houses. To clarify, **UNSAFE** is only able to estimate damages for these structures right now, so this subset step is required. We are working to make UNSAFE applicable to a larger set of structure types but are limited by the few DDFs that account for uncertainty in depth-damage relationships. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014c5353-2508-4708-b3bf-b3f624fe9350",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsi_gdf = unexp.get_nsi_geo(FIPS, NSI_CRS, EXP_DIR_R)\n",
    "\n",
    "# Set the values that we pass into the get_struct_subset\n",
    "# function. In this case, occtype==RES1 and num_story <= 2\n",
    "occtype_list=['RES1-1SNB', 'RES1-2SNB', 'RES1-1SWB', 'RES1-2SWB']\n",
    "sub_string = 'occtype.isin(@occtype_list) and num_story <= 2'\n",
    "\n",
    "nsi_sub = unexp.get_struct_subset(nsi_gdf,\n",
    "                                  filter=sub_string,\n",
    "                                  occtype_list=occtype_list)\n",
    "\n",
    "# For this case study, let us save some memory and just\n",
    "# write out the single family houses that we just\n",
    "# subset. You could write out the nsi_gdf GeoDataFrame\n",
    "# if you'd like to. \n",
    "EXP_OUT_FILEP = join(EXP_DIR_I, FIPS, 'nsi_sf.gpkg')\n",
    "unfile.prepare_saving(EXP_OUT_FILEP)\n",
    "nsi_sub.to_file(EXP_OUT_FILEP, driver='GPKG')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c70cd6-09e4-4f32-a4ff-3a4851e30ca6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T21:26:57.263500Z",
     "iopub.status.busy": "2024-03-20T21:26:57.262895Z",
     "iopub.status.idle": "2024-03-20T21:27:38.485053Z",
     "shell.execute_reply": "2024-03-20T21:27:38.484223Z",
     "shell.execute_reply.started": "2024-03-20T21:26:57.263447Z"
    }
   },
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6451c49d",
   "metadata": {},
   "source": [
    "We process geospatial reference data (like census tracts and block groups) for three reasons:\n",
    "1) We will use census tracts to define the unit of aggregation for specifying multinomial distributions for foundation type and number of stories; and\n",
    "2) Data on social vulnerability, like the Climate and Economic Justice Screening Tool, are available as tabular data so we will link these to the corresponding geospatial reference dataset; and\n",
    "3) It can be useful to estimate losses at the property-level and then aggregate these estimates to different spatial scales. This is also a common use-case. \n",
    "\n",
    "We first define a clip_gdf GeoDataFrame reference to define our study's spatial extent. Here, we use the whole county, but users could upload a specific spatial boundary if they'd like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e55b1fc-9f4c-47b2-8daa-72613b672b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to clip reference data to a clip file that\n",
    "# represents our study boundaries. In this case, it's the county\n",
    "# of Philadelphia, so we will prepare that as our clip file\n",
    "county_filep = join(REF_DIR_UZ, NATION, 'county', 'tl_2022_us_county.shp')\n",
    "county_gdf = gpd.read_file(county_filep)\n",
    "clip_gdf = county_gdf[county_gdf[REF_ID_NAMES_DICT['county']] == FIPS]\n",
    "\n",
    "# clip_ref_files will go through all unzipped ref files,\n",
    "# clip them in the clip file geometry, and write them\n",
    "# We pass in arguments we defined in the configuration step\n",
    "# to tell the function where the data can be found (REF_DIR_UZ),\n",
    "# where is is going (REF_DIR_I), and how to update id names\n",
    "# (REF_NAMES_DICT). \n",
    "unexp.clip_ref_files(clip_gdf, FIPS,\n",
    "                     REF_DIR_UZ, REF_DIR_I, REF_NAMES_DICT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9eaafa-7a8f-469b-a64b-1737d3af4b04",
   "metadata": {},
   "source": [
    "## Physical vulnerability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4507b11c",
   "metadata": {},
   "source": [
    "This is where we call functions to prepare our DDFs for estimating losses under uncertainty. In some analyses, you may only want to use one set of DDFs, so you only have to call one process_DDF() function. In the future, there may be many more functions to draw from, so we decided the workflow would be most transparent if users had to explicitly call a function to process a particular set of DDFs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c642ef35",
   "metadata": {},
   "source": [
    "We have separate functions for each DDF because the datasets have slightly different structures and processing requirements. HAZUS DDFs do not have uncertainty 'baked in' so we add noise following [previous research](https://www.nature.com/articles/s41467-020-19188-9) for this case study. You can update the parameter that adds noise and see how it changes the damage estimate distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c538bf-ee9f-485c-a7f3-df85862107ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "unddf.process_naccs(VULN_DIR_UZ, VULN_DIR_I)\n",
    "\n",
    "UNIF_UNC = .3\n",
    "unddf.process_hazus(VULN_DIR_UZ, VULN_DIR_I, unif_unc=UNIF_UNC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cb7e13-17c6-4877-b7af-45ea26eaf385",
   "metadata": {},
   "source": [
    "## Social vulnerability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48da74e1",
   "metadata": {},
   "source": [
    "UNSAFE can ingest CDC Socially Vulnerable Index data and Climate and Economic Justice and Screening Tool data. Users do not have to process data about social vulnerability, but it is a common use case. As such, we developed UNSAFE with preliminary functionality for this type of usage. The function we call below calls subroutines that have code to handle the unique features of the different data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad1b1b6-c7fb-4639-9d79-0f61f21bbaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process national social vulnerability data\n",
    "# Tell the function which datasets we want processed\n",
    "# In this case study, we will use cejst and svi\n",
    "# which are available nationally and we specified in\n",
    "# our DOWNLOAD configuration dictionary. \n",
    "\n",
    "sovi_list = ['cejst', 'svi']\n",
    "unexp.process_national_sovi(sovi_list, FIPS,\n",
    "                            VULN_DIR_R, REF_DIR_I, VULN_DIR_I)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6d3425-ba65-489d-8edf-f9c0ae862a67",
   "metadata": {},
   "source": [
    "## Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa129d27",
   "metadata": {},
   "source": [
    "The National Flood Hazard Layer is required for using HAZUS DDFs because these apply different DDF relationships based on whether a house is in the V zone. In our case study, all houses are in the A or X zone, but this is a core function that is needed for other case studies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee5dc31-081a-4c17-b5b5-004dce20ea20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need NFHL for the ensemble and visualizations\n",
    "unexp.process_nfhl(FIPS,\n",
    "                   POL_DIR_UZ,\n",
    "                   POL_DIR_I)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56c2f93-c107-436b-ac1e-3648b162b64e",
   "metadata": {},
   "source": [
    "## Link flood zones and references to structures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699379b0",
   "metadata": {},
   "source": [
    "We will now link our GeoDataFrame of single family structures to the other spatial data we processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd28ca6-90bb-431a-bab6-af1953ce8e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link flood zones\n",
    "# We checked for issues like overlapping flood zones\n",
    "# resulting in NSI structures in multiple polygons\n",
    "# and did not find any. That's good, but chances\n",
    "# are there will be counties where this happens\n",
    "# and we will need code to handle these consistently for other\n",
    "# case studies\n",
    "nfhl_filep = join(POL_DIR_I, FIPS, 'fld_zones.gpkg')\n",
    "nfhl = gpd.read_file(nfhl_filep)\n",
    "keep_cols = ['fld_zone', 'fld_ar_id', 'static_bfe']\n",
    "\n",
    "# This function is designed to work in general\n",
    "# for any spatial-based data you want to link\n",
    "# to structures. We demonstrate it with the\n",
    "# nfhl data. \n",
    "unexp.get_spatial_var(nsi_sub,\n",
    "                      nfhl,\n",
    "                      'fz',\n",
    "                      FIPS,\n",
    "                      EXP_DIR_I,\n",
    "                      keep_cols)\n",
    "\n",
    "# Link references\n",
    "# This will do spatial joins for structures within\n",
    "# all the reference spatial files (besides county)\n",
    "# and output a file of fd_id (these are unique strucutre ids)\n",
    "# linked to all of the reference ids\n",
    "unexp.get_ref_ids(nsi_sub, FIPS,\n",
    "                  REF_ID_NAMES_DICT, REF_DIR_I, EXP_DIR_I)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b812a394-ad51-4bde-9589-07f9259d1adb",
   "metadata": {},
   "source": [
    "## Hazard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b3f6f8",
   "metadata": {},
   "source": [
    "This is where we link flood depths to structures. We are using the FEMA Flood Risk Database which consists of the 500, 100, 50, and 10 year return periods. We specified in our configuration file that we are only interested in the depth grids corresponding to inland flooding. The purpose of this function is to do point-in-raster sampling for the depth grids indicated by the RET_PERS and HAZ_FILN arguments. Users can add pre-processing code that is unique to the hazard data they are bringing into the UNSAFE framework to call the `get_inundations()` function. For example, if you want to try experimenting with the `full_data_example.ipynb` you can try adding pre-processing code that selects the max inundation across the inland and coastal depth grids in the Flood Risk Database, and then modify the HAZ_FILEN argument to link these depths to the structures. Or, you can modify the RET_PERS argument to serve more generally as a flood scenario indicator and simultaneously modify the HAZ_FILN argument to accommodate reading in the correct files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2250956b-b979-4122-8695-dd9a5dde870f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the inundation grids and write out the\n",
    "# fd_id/depths dataframe\n",
    "\n",
    "# This code looks different than the full data example\n",
    "# because we created the zip directory in a different way\n",
    "# than the zip directory you download from the fema\n",
    "# risk database\n",
    "haz_dir_uz_clip = join(HAZ_DIR_UZ, 'dg_clipped')\n",
    "depth_df = unexp.get_inundations(nsi_sub,\n",
    "                                 HAZ_CRS, RET_PERS,\n",
    "                                 haz_dir_uz_clip, HAZ_FILEN)\n",
    "\n",
    "# Because we are processing design-event based flood scenarios,\n",
    "# we can provide more helpful column names to prepare our loss\n",
    "# and expected annual loss calculations. We will also\n",
    "# write out our dataframe. \n",
    "ncol = [str(round(100/float(x.replace('_', '.')))) for x in depth_df.columns]\n",
    "depth_df.columns = ncol\n",
    "\n",
    "# Write out dataframe that links fd_id to depths\n",
    "# with columns corresponding to ret_per (i.e. 500, 100, 50, 10)\n",
    "# in our case study\n",
    "nsi_depths_out = join(EXP_DIR_I, FIPS, 'nsi_depths.pqt')\n",
    "unfile.prepare_saving(nsi_depths_out)\n",
    "depth_df.reset_index().to_parquet(nsi_depths_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b6a614-1687-4963-a28a-ba00d423485c",
   "metadata": {},
   "source": [
    "# Generate ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d093730c",
   "metadata": {},
   "source": [
    "This corresponds to steps 10-12 in the workflow. We include example code for estimating losses without uncertainty on the base dataframe that is used for generating the ensemble. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72621f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a dataframe conducive for loss estimation\n",
    "# This procedure is separate from preparing data for the ensemble\n",
    "# so will just take the county code to load in and merge\n",
    "# all the relevant data\n",
    "\n",
    "base_df = unens.get_base_df(FIPS, EXP_DIR_I)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf3fe45",
   "metadata": {},
   "source": [
    "This is the core of what **UNSAFE** does. We generate ensemble members (sometimes called states of the world, or SOWs) based on user defined inputs. Users pass nsi_sub, the full dataframe of single family structures, to help the function learn the proportions of structure types at census tract scale across a case study. They also pass base_df that stores valuable information about the houses at risk of flooding in the case study. Then, users pass a list of DDFs and structure types. In this case study, we estimate damages for NACCS and HAZUS DDFs with uncertainty in first-floor elevation, structure value, number of stories, and basement type. The other arguments are defined in our configuration file. N_SOW is the number of ensemble members, or how many times we sample from uncertain distributions. FFE_DICT stores the triangular distributions for first-floor elevation, conditioned on different foundation types. And COEF_VARIATION is how we define noise around the NSI structure values. Finally, VULN_DIR_I is a directory path for where we put our DDFs that are ready to estimate losses under uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96915863-55ed-4e8d-9a70-6b673105e6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ens_df_f = unens.generate_ensemble(nsi_sub,\n",
    "                                   base_df,\n",
    "                                   ['hazus', 'naccs'],\n",
    "                                   ['ffe', 'val_struct', 'stories', 'basement'],\n",
    "                                   N_SOW,\n",
    "                                   FFE_DICT,\n",
    "                                   COEF_VARIATION,\n",
    "                                   VULN_DIR_I)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a12813",
   "metadata": {},
   "source": [
    "Because the flood events we used are defined as return periods, we can estimate expected annual loss. We do subsetting the loss columns and calling the `get_eal()` function on an ordered list from most to least frequent return period (e.g., 10 year event is ordered before the 500 year event). It is easiest to do this by looping through the damage functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eals = {}\n",
    "for ddf in ['hazus', 'naccs']:\n",
    "    col_sub = [x for x in ens_df_f if ddf + '_loss' in x]\n",
    "    loss_sub = ens_df_f[col_sub]\n",
    "    # Update column name to remove reference to ddf\n",
    "    loss_sub.columns = ['_'.join(x.split('_')[1:]) for x in col_sub]\n",
    "    ret_per_ints = [int(x.split('_')[-1]) for x in col_sub]\n",
    "    rp_list = sorted(ret_per_ints)\n",
    "    eals[ddf] = unddf.get_eal(loss_sub, rp_list)\n",
    "eals = pd.DataFrame.from_dict(eals)\n",
    "eals.columns = [x + '_eal' for x in eals.columns]\n",
    "ens_df_out = pd.concat([ens_df_f, eals], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7029b3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframes\n",
    "base_out_filep = join(FO, 'base_df.pqt')\n",
    "unfile.prepare_saving(base_out_filep)\n",
    "base_df.to_parquet(base_out_filep)\n",
    "\n",
    "ens_out_filep = join(FO, 'ensemble.pqt')\n",
    "ens_df_out.to_parquet(ens_out_filep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063969aa-0b7c-4078-9e6d-37b2d5b8cbb0",
   "metadata": {},
   "source": [
    "## Estimate benchmark losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b11d23-3c34-472a-a36f-6f06d94daaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also want benchmark estimates without uncertainty \n",
    "# which we can do with the full_df specified above\n",
    "nounc_df = unens.benchmark_loss(base_df, VULN_DIR_I)\n",
    "\n",
    "hazus_def_out_filep = join(FO, 'benchmark_loss.pqt')\n",
    "unfile.prepare_saving(hazus_def_out_filep)\n",
    "nounc_df.to_parquet(hazus_def_out_filep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873cfd06-8977-466d-9191-39eab0af37dd",
   "metadata": {},
   "source": [
    "# Checking aggregate results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bb7cfe",
   "metadata": {},
   "source": [
    "You can run the code below to compare the histograms of the expected annual loss estimates for both damage functions. You will not get the exact same result each time you run this notebook, but you should get something very similar! You can compare your output to the picture in `examples/phil_frd_partial/fig/example_output.png` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fda306-01c2-403f-92a1-d1961c76314b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Patch\n",
    "import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6), dpi=300,\n",
    "                       nrows=2,\n",
    "                       sharex=True,\n",
    "                       gridspec_kw={'hspace': 0},\n",
    "                       height_ratios=[1,3])\n",
    "# When we groupby 'sow_ind' we are looking across ensemble members\n",
    "temp = ens_df_out.groupby(['sow_ind'])[['naccs_eal', 'hazus_eal']].sum()/1e6\n",
    "temp['naccs_eal'].hist(bins=30, color='blue', alpha=.5, label='NACCS DDFs')\n",
    "temp['hazus_eal'].hist(bins=30, color='orange', alpha=.5, label='HAZUS DDFs')\n",
    "ax[1].axvline(nounc_df['eal'].sum()/1e6, color='red', label='No Uncertainty')\n",
    "ax[0].axvline(nounc_df['eal'].sum()/1e6, color='red', label='No Uncertainty')\n",
    "ax[1].grid(False)\n",
    "ax[1].set_xlabel('Total Expected Annual Loss ($ Millions)', size=14)\n",
    "ax[1].set_ylabel('Number of Ensemble Members', size=14)\n",
    "ax[1].tick_params(labelsize=12)\n",
    "\n",
    "temp.columns = ['NACCS', 'HAZUS']\n",
    "temp_box = temp.melt(value_name='eal',\n",
    "                     var_name='DDF Type')\n",
    "sns.boxplot(ax=ax[0],\n",
    "            data=temp_box,\n",
    "            x='eal',\n",
    "            hue='DDF Type',\n",
    "            legend=False,\n",
    "            showmeans=True,\n",
    "            meanprops={'markerfacecolor': 'firebrick',\n",
    "                        'markeredgecolor': 'black',\n",
    "                        'marker': 'D'})\n",
    "ax[0].axis('off')\n",
    "\n",
    "\n",
    "# Easier to do a custom legend\n",
    "legend_elements = [Patch(facecolor=sns.color_palette(\"tab10\")[1],\n",
    "                         label='HAZUS DDFs'),\n",
    "                    Patch(facecolor=sns.color_palette(\"tab10\")[0],\n",
    "                         label='NACCS DDFs'),\n",
    "                   Line2D([0], [0], color='r', lw=2, label='No Uncertainty'),\n",
    "                   Line2D([0], [0], marker='D', markerfacecolor='firebrick',\n",
    "                          label='Ensemble Mean',\n",
    "                          ls='',\n",
    "                          markeredgecolor='black', markersize=8),]\n",
    "\n",
    "\n",
    "ax[1].legend(handles=legend_elements,\n",
    "             loc='upper right',\n",
    "             fontsize='x-large',\n",
    "             frameon=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsafe01",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
