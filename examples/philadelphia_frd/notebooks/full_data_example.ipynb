{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed264f1c-60c1-49bd-af7b-ca42fd88e194",
   "metadata": {},
   "source": [
    "# Configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedcdf6b-f5df-4b72-b285-3313df884f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cbc7fd-af6d-4800-86c4-4ea008bf85d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import os\n",
    "from os.path import join\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "from yaml.loader import SafeLoader\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "\n",
    "import unsafe.download as undown\n",
    "import unsafe.files as unfile\n",
    "import unsafe.unzip as ununzip\n",
    "import unsafe.exp as unexp\n",
    "import unsafe.ddfs as unddf\n",
    "import unsafe.ensemble as unens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58884488-133d-4dd2-9d44-6fba2ac1526a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name the fips, statefips, stateabbr, and nation that\n",
    "# we are using for this analysis\n",
    "# We pass these in as a list even though the framework currently\n",
    "# processes a single county so that it can facilitate that\n",
    "# expansion in the future\n",
    "# TODO - could make sense to define these in the future\n",
    "# in json or other formats instead of as input in code\n",
    "fips_args = {\n",
    "    'FIPS': ['42101'], \n",
    "    'STATEFIPS': ['42'],\n",
    "    'STATEABBR': ['PA'],\n",
    "    'NATION': ['US']\n",
    "}\n",
    "FIPS = fips_args['FIPS'][0]\n",
    "NATION = fips_args['NATION'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad060e58-0489-4bf4-8e24-6728dd15c0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to pass in a config file that sets up\n",
    "# constants and the structure for downlading data\n",
    "# For the directory structure of our case study, \n",
    "# we use the following \n",
    "ABS_DIR = Path().absolute().parents[0]\n",
    "\n",
    "CONFIG_FILEP = join(ABS_DIR, 'config', 'config.yaml')\n",
    "# Open the config file and load\n",
    "with open(CONFIG_FILEP) as f:\n",
    "    CONFIG = yaml.load(f, Loader=SafeLoader)\n",
    "\n",
    "# Wildcards for urls\n",
    "URL_WILDCARDS = CONFIG['url_wildcards']\n",
    "\n",
    "# Get the file extensions for api endpoints\n",
    "API_EXT = CONFIG['api_ext']\n",
    "\n",
    "# Get the CRS constants\n",
    "NSI_CRS = CONFIG['nsi_crs']\n",
    "\n",
    "# Dictionary of ref_names\n",
    "REF_NAMES_DICT = CONFIG['ref_names']\n",
    "\n",
    "# Dictionary of ref_id_names\n",
    "REF_ID_NAMES_DICT = CONFIG['ref_id_names']\n",
    "\n",
    "# Coefficient of variation\n",
    "# for structure values\n",
    "COEF_VARIATION = CONFIG['coef_var']\n",
    "\n",
    "# First floor elevation dictionary\n",
    "FFE_DICT = CONFIG['ffe_dict']\n",
    "\n",
    "# Number of states of the world\n",
    "N_SOW = CONFIG['sows']\n",
    "\n",
    "# Get hazard model variables\n",
    "# Get Return Period list\n",
    "RET_PERS = CONFIG['RPs']\n",
    "HAZ_FILEN = CONFIG['haz_filename']\n",
    "# Get CRS for depth grids\n",
    "HAZ_CRS = CONFIG['haz_crs']\n",
    "\n",
    "# Get the files we need downloaded\n",
    "DOWNLOAD = pd.json_normalize(CONFIG['download'], sep='_').T\n",
    "\n",
    "# We can also specify the filepath to the\n",
    "# raw data directory\n",
    "FR = join(ABS_DIR, \"data\", \"raw\")\n",
    "\n",
    "# And external - where our hazard data should be\n",
    "FE = join(FR, \"external\")\n",
    "\n",
    "# Set up interim and results directories as well\n",
    "# We already use \"FR\" for raw, we use \"FO\" \n",
    "# because you can also think of results\n",
    "# as output\n",
    "FI = join(ABS_DIR, \"data\", \"interim\")\n",
    "FO = join(ABS_DIR, \"data\", \"results\")\n",
    "\n",
    "# \"Raw\" data directories for exposure, vulnerability (vuln) and\n",
    "# administrative reference files\n",
    "EXP_DIR_R = join(FR, \"exp\")\n",
    "VULN_DIR_R = join(FR, \"vuln\")\n",
    "REF_DIR_R = join(FR, \"ref\")\n",
    "# Haz is for depth grids\n",
    "HAZ_DIR_R = join(FE, \"haz\")\n",
    "# Pol is for NFHL\n",
    "POL_DIR_R = join(FR, \"pol\")\n",
    "\n",
    "# Unzip directory \n",
    "UNZIP_DIR = join(FR, \"unzipped\")\n",
    "\n",
    "# We want to process unzipped data and move it\n",
    "# to the interim directory where we keep\n",
    "# processed data\n",
    "# Get the filepaths for unzipped data\n",
    "# We unzipped the depth grids (haz) and \n",
    "# ddfs (vuln) into the \"external\"/ subdirectory\n",
    "HAZ_DIR_UZ = join(UNZIP_DIR, \"external\", \"haz\")\n",
    "POL_DIR_UZ = join(UNZIP_DIR, \"pol\")\n",
    "REF_DIR_UZ = join(UNZIP_DIR, \"ref\")\n",
    "VULN_DIR_UZ = join(UNZIP_DIR, \"external\", \"vuln\")\n",
    "\n",
    "# \"Interim\" data directories\n",
    "EXP_DIR_I = join(FI, \"exp\")\n",
    "VULN_DIR_I = join(FI, \"vuln\")\n",
    "REF_DIR_I = join(FI, \"ref\")\n",
    "# Haz is for depth grids\n",
    "HAZ_DIR_I = join(FI, \"haz\")\n",
    "# Pol is for NFHL\n",
    "POL_DIR_I = join(FI, \"pol\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1416bf35-5d86-45c8-9869-617e4709f46a",
   "metadata": {},
   "source": [
    "# Download (and unzip) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1cd00d-1c06-43ac-ade6-dc949e99d8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "wcard_dict = {x: fips_args[x[1:-1]][0] for x in URL_WILDCARDS}\n",
    "undown.download_raw(DOWNLOAD, wcard_dict,\n",
    "                    FR, API_EXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd95446c-adaa-4252-b94c-4adbe1e75a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "ununzip.unzip_raw(FR, UNZIP_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d5b9de-7fea-4474-ae90-47127e366661",
   "metadata": {},
   "source": [
    "# Prepare data for ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2167d3cf-adf4-4b5e-80e7-2db7873641cb",
   "metadata": {},
   "source": [
    "## Exposure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014c5353-2508-4708-b3bf-b3f624fe9350",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsi_gdf = unexp.get_nsi_geo(FIPS, NSI_CRS, EXP_DIR_R)\n",
    "\n",
    "# Set the values that we pass into the get_struct_subset\n",
    "# function. In this case, occtype==RES1 and num_story <= 2\n",
    "occtype_list=['RES1-1SNB', 'RES1-2SNB', 'RES1-1SWB', 'RES1-2SWB']\n",
    "sub_string = 'occtype.isin(@occtype_list) and num_story <= 2'\n",
    "nsi_sub = unexp.get_struct_subset(nsi_gdf,\n",
    "                                  filter=sub_string,\n",
    "                                  occtype_list=occtype_list)\n",
    "\n",
    "# For this case study, let us save some memory and just\n",
    "# write out the single family houses \n",
    "EXP_OUT_FILEP = join(EXP_DIR_I, FIPS, 'nsi_sf.gpkg')\n",
    "unfile.prepare_saving(EXP_OUT_FILEP)\n",
    "nsi_sub.to_file(EXP_OUT_FILEP, driver='GPKG')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c70cd6-09e4-4f32-a4ff-3a4851e30ca6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T21:26:57.263500Z",
     "iopub.status.busy": "2024-03-20T21:26:57.262895Z",
     "iopub.status.idle": "2024-03-20T21:27:38.485053Z",
     "shell.execute_reply": "2024-03-20T21:27:38.484223Z",
     "shell.execute_reply.started": "2024-03-20T21:26:57.263447Z"
    }
   },
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e55b1fc-9f4c-47b2-8daa-72613b672b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to clip reference data to a clip file that\n",
    "# represents our study boundaries. In this case, it's the county\n",
    "# of Philadelphia, so we will prepare that as our clip file\n",
    "county_filep = join(REF_DIR_UZ, NATION, 'county', 'tl_2022_us_county.shp')\n",
    "county_gdf = gpd.read_file(county_filep)\n",
    "clip_gdf = county_gdf[county_gdf[REF_ID_NAMES_DICT['county']] == FIPS]\n",
    "\n",
    "# clip_ref_files will go through all unzipped ref files,\n",
    "# clip them in the clip file geometry, and write them\n",
    "# We pass in arguments we defined in the configuration step\n",
    "# to tell the function where the data can be found (REF_DIR_UZ),\n",
    "# where is is going (REF_DIR_I), and how to update id names\n",
    "# (REF_NAMES_DICT). \n",
    "# Subset of ref downloads\n",
    "ref_downloads = DOWNLOAD[DOWNLOAD.index.str.contains('_ref_')]\n",
    "# Clip ref to catchment\n",
    "# Use FIPS as clip_str since the catchment is in a county\n",
    "# and this is eaiser for directory management\n",
    "unexp.clip_ref_files(clip_gdf, FIPS, fips_args, ref_downloads,\n",
    "                     wcard_dict, REF_DIR_UZ, REF_DIR_I)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9eaafa-7a8f-469b-a64b-1737d3af4b04",
   "metadata": {},
   "source": [
    "## Physical vulnerability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c538bf-ee9f-485c-a7f3-df85862107ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "unddf.process_naccs(VULN_DIR_UZ, VULN_DIR_I)\n",
    "\n",
    "# .3 was used in Zarekarizi et al. 2020\n",
    "# https://www.nature.com/articles/s41467-020-19188-9\n",
    "# and we are going to use that for this case study\n",
    "UNIF_UNC = .3\n",
    "unddf.process_hazus(VULN_DIR_UZ, VULN_DIR_I, unif_unc=UNIF_UNC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cb7e13-17c6-4877-b7af-45ea26eaf385",
   "metadata": {},
   "source": [
    "## Social vulnerability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad1b1b6-c7fb-4639-9d79-0f61f21bbaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process national social vulnerability data\n",
    "# Tell the function which datasets we want processed\n",
    "# In this case study, we will use cejst\n",
    "# which is still available nationally\n",
    "\n",
    "sovi_list = ['cejst']\n",
    "unexp.process_national_sovi(sovi_list, FIPS,\n",
    "                            VULN_DIR_R, REF_DIR_I, VULN_DIR_I)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6d3425-ba65-489d-8edf-f9c0ae862a67",
   "metadata": {},
   "source": [
    "## Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee5dc31-081a-4c17-b5b5-004dce20ea20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need NFHL for the ensemble and visualizations\n",
    "unexp.process_nfhl(FIPS,\n",
    "                   POL_DIR_UZ,\n",
    "                   POL_DIR_I,\n",
    "                   \"Philadelphia_NFHL_42_2023.shp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56c2f93-c107-436b-ac1e-3648b162b64e",
   "metadata": {},
   "source": [
    "## Link flood zones and references to structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd28ca6-90bb-431a-bab6-af1953ce8e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link flood zones\n",
    "# We checked for issues like overlapping flood zones\n",
    "# resulting in NSI structures in multiple polygons\n",
    "# and did not find any. That's good, but chances\n",
    "# are there will be counties where this happens\n",
    "# and we will need code to handle these consistently for other\n",
    "# case studies\n",
    "nfhl_filep = join(POL_DIR_I, FIPS, 'fld_zones.gpkg')\n",
    "nfhl = gpd.read_file(nfhl_filep)\n",
    "keep_cols = ['fld_zone', 'fld_ar_id', 'static_bfe']\n",
    "\n",
    "# This function is designed to work in general\n",
    "# for any spatial-based data you want to link\n",
    "# to structures. We demonstrate it with the\n",
    "# nfhl data. \n",
    "unexp.get_spatial_var(nsi_sub,\n",
    "                      nfhl,\n",
    "                      'fz',\n",
    "                      FIPS,\n",
    "                      EXP_DIR_I,\n",
    "                      keep_cols)\n",
    "\n",
    "# Link references\n",
    "# This will do spatial joins for structures within\n",
    "# all the reference spatial files (besides county)\n",
    "# and output a file of fd_id (these are unique strucutre ids)\n",
    "# linked to all of the reference ids\n",
    "nsi_refs = unexp.get_ref_ids(nsi_sub.set_index('fd_id'), FIPS,\n",
    "                             REF_ID_NAMES_DICT, REF_DIR_I, EXP_DIR_I)\n",
    "\n",
    "nsi_ref_filep = join(EXP_DIR_I, FIPS, \"nsi_ref.pqt\")\n",
    "unfile.prepare_saving(nsi_ref_filep)\n",
    "nsi_refs.to_parquet(nsi_ref_filep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b812a394-ad51-4bde-9589-07f9259d1adb",
   "metadata": {},
   "source": [
    "## Hazard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2250956b-b979-4122-8695-dd9a5dde870f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the inundation grids and write out the\n",
    "# fd_id/depths dataframe\n",
    "depth_df = unexp.get_inundations(nsi_sub,\n",
    "                                 HAZ_CRS, RET_PERS,\n",
    "                                 HAZ_DIR_UZ, HAZ_FILEN)\n",
    "\n",
    "# Because we are processing design-event based flood scenarios,\n",
    "# we can provide more helpful column names to prepare our loss\n",
    "# and expected annual loss calculations. We will also\n",
    "# write out our dataframe. \n",
    "ncol = [str(round(100/float(x.replace('_', '.')))) for x in depth_df.columns]\n",
    "depth_df.columns = ncol\n",
    "\n",
    "# Write out dataframe that links fd_id to depths\n",
    "# with columns corresponding to ret_per (i.e. 500, 100, 50, 10)\n",
    "# in our case study\n",
    "nsi_depths_out = join(EXP_DIR_I, FIPS, 'nsi_depths.pqt')\n",
    "unfile.prepare_saving(nsi_depths_out)\n",
    "depth_df.reset_index().to_parquet(nsi_depths_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b6a614-1687-4963-a28a-ba00d423485c",
   "metadata": {},
   "source": [
    "# Generate ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96915863-55ed-4e8d-9a70-6b673105e6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a dataframe conducive for loss estimation\n",
    "# This procedure is separate from preparing data for the ensemble\n",
    "# so will just take the county code to load in and merge\n",
    "# all the relevant data\n",
    "\n",
    "run_config = {\n",
    "    'ddfs': ['hazus', 'naccs'],\n",
    "    'struct_list': ['ffe', 'val_struct', 'num_story', 'found_type'],\n",
    "    'base_adj': False\n",
    "}\n",
    "\n",
    "base_df = nsi_sub.set_index('fd_id')\n",
    "# We only want the RES1 classification from the NSI\n",
    "# for damage function assignments\n",
    "# (not their 1SWB, etc., type classifications)\n",
    "base_df['occtype'] = base_df['occtype'].str[:4].copy()\n",
    "# We can easily read in our census tract links from above,\n",
    "# but we use this shortcut in the interest\n",
    "# of keeping the tutorial more readable and \n",
    "# focused on functionality\n",
    "base_df['tract_id'] = base_df['cbfips'].str[:11]\n",
    "# For HAZUS DDFs, we need to link a property to its\n",
    "# flood zone. Since Philadelphia is not subject to \n",
    "# wave action, we automatically use the A zone\n",
    "# curves (as opposed to Z). For other\n",
    "# case studies, if V zones are appropriate,\n",
    "# you need to merge in the flood zone data\n",
    "# we linked to parcels above\n",
    "base_df['fz_ddf'] = 'A'\n",
    "\n",
    "# We demonstrate the ensemble generation when all sources of uncertainty vary\n",
    "ens_df_f = unens.get_loss_ensemble(base_df,\n",
    "                                   depth_df,\n",
    "                                   run_config,\n",
    "                                   VULN_DIR_I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ebea81",
   "metadata": {},
   "outputs": [],
   "source": [
    "eals = {}\n",
    "for ddf in ['hazus', 'naccs']:\n",
    "    col_sub = [x for x in ens_df_f if ddf + '_loss' in x]\n",
    "    loss_sub = ens_df_f[col_sub]\n",
    "    # Update column name to remove reference to ddf\n",
    "    loss_sub.columns = ['_'.join(x.split('_')[1:]) for x in col_sub]\n",
    "    ret_per_ints = [int(x.split('_')[-1]) for x in col_sub]\n",
    "    rp_list = sorted(ret_per_ints)\n",
    "    eals[ddf] = unddf.get_eal(loss_sub, rp_list)\n",
    "eals = pd.DataFrame.from_dict(eals)\n",
    "eals.columns = [x + '_eal' for x in eals.columns]\n",
    "ens_df_out = pd.concat([ens_df_f, eals], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8326a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframes\n",
    "base_out_filep = join(FO, 'base_df.pqt')\n",
    "unfile.prepare_saving(base_out_filep)\n",
    "base_df.to_parquet(base_out_filep)\n",
    "\n",
    "ens_out_filep = join(FO, 'ensemble.pqt')\n",
    "ens_df_out.to_parquet(ens_out_filep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063969aa-0b7c-4078-9e6d-37b2d5b8cbb0",
   "metadata": {},
   "source": [
    "# Estimate benchmark losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b11d23-3c34-472a-a36f-6f06d94daaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also want benchmark estimates without uncertainty \n",
    "# which we can do with the full_df specified above\n",
    "nounc_df = unens.benchmark_loss(base_df, depth_df, VULN_DIR_I, base_adj=False)\n",
    "rp_list = sorted([int(x.split(\"_\")[-1]) for x in nounc_df.columns])\n",
    "eals = unddf.get_eal(nounc_df, rp_list)\n",
    "\n",
    "nounc_df = pd.concat([nounc_df, pd.Series(eals, name='eal')], axis=1)\n",
    "\n",
    "hazus_def_out_filep = join(FO, 'benchmark_loss.pqt')\n",
    "unfile.prepare_saving(hazus_def_out_filep)\n",
    "nounc_df.to_parquet(hazus_def_out_filep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873cfd06-8977-466d-9191-39eab0af37dd",
   "metadata": {},
   "source": [
    "# Example plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed97908a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Patch\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4afe90c",
   "metadata": {},
   "source": [
    "Read the loss estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04744754",
   "metadata": {},
   "outputs": [],
   "source": [
    "nounc_df = pd.read_parquet(join(FO, 'benchmark_loss.pqt'))\n",
    "ens_df_f = pd.read_parquet(join(FO, 'ensemble.pqt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc7b620",
   "metadata": {},
   "source": [
    "Compare the no uncertainty estimate to the distributions from estimating damages under uncertainty with two different damage functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fda306-01c2-403f-92a1-d1961c76314b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6), dpi=300,\n",
    "                       nrows=2,\n",
    "                       sharex=True,\n",
    "                       gridspec_kw={'hspace': 0},\n",
    "                       height_ratios=[1,3])\n",
    "# When we groupby 'sow_ind' we are looking across ensemble members\n",
    "temp = ens_df_f.groupby(['sow_ind'])[['naccs_eal', 'hazus_eal']].sum()/1e6\n",
    "temp['naccs_eal'].hist(bins=50, color='blue', alpha=.5, label='NACCS DDFs')\n",
    "temp['hazus_eal'].hist(bins=50, color='orange', alpha=.5, label='HAZUS DDFs')\n",
    "ax[1].axvline(nounc_df['eal'].sum()/1e6, color='red', label='No Uncertainty')\n",
    "ax[0].axvline(nounc_df['eal'].sum()/1e6, color='red', label='No Uncertainty')\n",
    "ax[1].grid(False)\n",
    "ax[1].set_xlabel('Total Expected Annual Loss ($ Millions)', size=14)\n",
    "ax[1].set_ylabel('Number of Ensemble Members', size=14)\n",
    "ax[1].tick_params(labelsize=12)\n",
    "\n",
    "temp.columns = ['NACCS', 'HAZUS']\n",
    "temp_box = temp.melt(value_name='eal',\n",
    "                     var_name='DDF Type')\n",
    "sns.boxplot(ax=ax[0],\n",
    "            data=temp_box,\n",
    "            x='eal',\n",
    "            hue='DDF Type',\n",
    "            legend=False,\n",
    "            showmeans=True,\n",
    "            meanprops={'markerfacecolor': 'firebrick',\n",
    "                        'markeredgecolor': 'black',\n",
    "                        'marker': 'D'})\n",
    "ax[0].axis('off')\n",
    "\n",
    "\n",
    "# Easier to do a custom legend\n",
    "legend_elements = [Patch(facecolor=sns.color_palette(\"tab10\")[1],\n",
    "                         label='HAZUS DDFs'),\n",
    "                    Patch(facecolor=sns.color_palette(\"tab10\")[0],\n",
    "                         label='NACCS DDFs'),\n",
    "                   Line2D([0], [0], color='r', lw=2, label='No Uncertainty'),\n",
    "                   Line2D([0], [0], marker='D', markerfacecolor='firebrick',\n",
    "                          label='Ensemble Mean',\n",
    "                          ls='',\n",
    "                          markeredgecolor='black', markersize=8),]\n",
    "\n",
    "\n",
    "ax[1].legend(handles=legend_elements,\n",
    "             loc='upper right',\n",
    "             fontsize='x-large',\n",
    "             frameon=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a0918c-5feb-41c9-80fb-103f3e027c6c",
   "metadata": {},
   "source": [
    "Can do the same for a single house."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc255ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the above for a house with some zero EAL \n",
    "# to demonstrate some ways to evaluate ensemble results\n",
    "\n",
    "check_prop_id = ens_df_f[ens_df_f['hazus_eal'] == 0]['fd_id'].unique()\n",
    "check_prop = ens_df_f[ens_df_f['fd_id'].isin(check_prop_id)]\n",
    "\n",
    "max_eal_id = check_prop.groupby('fd_id')['hazus_eal'].mean().idxmax()\n",
    "\n",
    "max_unc = ens_df_f[ens_df_f['fd_id'] == max_eal_id]\n",
    "max_nounc = nounc_df.loc[max_eal_id]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6), dpi=300,\n",
    "                       nrows=2,\n",
    "                       sharex=True,\n",
    "                       gridspec_kw={'hspace': 0},\n",
    "                       height_ratios=[1,3])\n",
    "\n",
    "temp = max_unc.groupby(['sow_ind'])[['naccs_eal', 'hazus_eal']].sum()/1e3\n",
    "\n",
    "temp['naccs_eal'].hist(bins=100, color='blue', alpha=.5, label='NACCS DDFs')\n",
    "temp['hazus_eal'].hist(bins=100, color='orange', alpha=.5, label='HAZUS DDFs')\n",
    "ax[1].axvline(max_nounc['eal'].sum()/1e3, color='red', label='No Uncertainty')\n",
    "ax[0].axvline(max_nounc['eal'].sum()/1e3, color='red', label='No Uncertainty')\n",
    "ax[1].grid(False)\n",
    "ax[1].set_xlabel('Expected Annual Loss ($ Thousands)', size=14)\n",
    "ax[1].set_ylabel('Number of Ensemble Members', size=14)\n",
    "ax[1].tick_params(labelsize=12)\n",
    "\n",
    "temp.columns = ['NACCS', 'HAZUS']\n",
    "temp_box = temp.melt(value_name='eal',\n",
    "                     var_name='DDF Type')\n",
    "sns.boxplot(ax=ax[0],\n",
    "            data=temp_box,\n",
    "            x='eal',\n",
    "            hue='DDF Type',\n",
    "            legend=False,\n",
    "            showmeans=True,\n",
    "            meanprops={'markerfacecolor': 'firebrick',\n",
    "                        'markeredgecolor': 'black',\n",
    "                        'marker': 'D'})\n",
    "ax[0].axis('off')\n",
    "\n",
    "\n",
    "\n",
    "# Easier to do a custom legend\n",
    "legend_elements = [Patch(facecolor=sns.color_palette(\"tab10\")[1],\n",
    "                         label='HAZUS DDFs'),\n",
    "                    Patch(facecolor=sns.color_palette(\"tab10\")[0],\n",
    "                         label='NACCS DDFs'),\n",
    "                   Line2D([0], [0], color='r', lw=2, label='No Uncertainty'),\n",
    "                   Line2D([0], [0], marker='D', markerfacecolor='firebrick',\n",
    "                          label='Ensemble Mean',\n",
    "                          ls='',\n",
    "                          markeredgecolor='black', markersize=8),]\n",
    "\n",
    "\n",
    "ax[1].legend(handles=legend_elements,\n",
    "             loc='upper right',\n",
    "             fontsize='x-large',\n",
    "             frameon=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d41b49",
   "metadata": {},
   "source": [
    "Interestingly, we see that there are some ensemble members with no (or little - hard to tell due to the grouping) expected annual loss, regardless of the DDF. In addition to this, there is evidence of two other modes in the distribution. We can look into the reasons for this. Let's check the realizations for foundation type and first-floor elevation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4e09e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To start, let's look at when exp. annual loss\n",
    "# is zero and non-zero when using hazus ddfs\n",
    "max_unc['Expected Annual Loss'] = np.where(max_unc['hazus_eal'] > 0,\n",
    "                                           '> 0',\n",
    "                                           '= 0')\n",
    "\n",
    "# We can bin the ffe column to make it easier to plot\n",
    "max_unc['ffe_bin'] = pd.cut(max_unc['ffe'], bins=[0, 1, 2, 3, 4])\n",
    "\n",
    "# Let's plot the first floor elevation on the x-axis\n",
    "# and let's plot foundation type in different\n",
    "# subplots. We will plot count on the y-axis.\n",
    "# We will do the bar colors based on whether\n",
    "# EAL = 0 or > 0\n",
    "sns.catplot(x='ffe_bin',\n",
    "            row='fnd_type',\n",
    "            data=max_unc,\n",
    "            hue='Expected Annual Loss',\n",
    "            kind='count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0434649",
   "metadata": {},
   "source": [
    "We see that very few of the ensemble members have zero expected annual loss. Those that do have crawl space foundation and first-floor elevation of 4 feet. It makes sense that houses with higher first-floor elevation will have less damage, all else equal. Let's look at boxplots for each foundation type/first-floor elevation combo to get a better handle of which ensemble members are driving the multi-modal loss distribution we saw. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b390c25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6), dpi=300,\n",
    "                       nrows=2,\n",
    "                       sharex=True,\n",
    "                       gridspec_kw={'hspace': 0},\n",
    "                       height_ratios=[1,3])\n",
    "\n",
    "g= sns.boxplot(data=max_unc,\n",
    "            x='ffe_bin', y='hazus_eal',\n",
    "            hue='fnd_type',\n",
    "            ax=ax[1])\n",
    "\n",
    "# replace legend labels\n",
    "new_labels = ['Slab', 'Basement', 'Crawl Space']\n",
    "for t, l in zip(g.legend_.texts, new_labels):\n",
    "    t.set_text(l)\n",
    "\n",
    "g= sns.countplot(data=max_unc,\n",
    "                 x='ffe_bin',\n",
    "                 hue='fnd_type',\n",
    "                 ax=ax[0],\n",
    "                 legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0805a0b3",
   "metadata": {},
   "source": [
    "The above diagnostic plot shows that slab and crawl space houses, with identifical first-floor elevation to basement houses, tend to have lower expected annual losses (with inundation equal). Since crawl space and slab houses are considered \"no basement\" they have different DDFs than basement houses, so this plot just reflects differences in the DDFs. We can see that crawl space houses at >= 2 feet, and some realizations of slab houses at 1 foot, account for the lower part of the bimodial histogram shown two plots ago. There is also a third mode, around $20,000 in the plot shown two figures ago. This appears to be driven by many of the realizations of slab and crawl space houses with 1 foot first-floor elevation, and the basement foundation realizations at 3 and 4 feet first-floor elevation. \n",
    "\n",
    "This set of diagnostics shows that even for the house with the highest expected annual loss, there is substantial uncertainty in expected annual loss estimation due to interacting uncertainty in features like foundation type and first-floor elevation. These are often not included in parcel datasets, and as highlighted in our technical documentation there is very little guidance on how accurate the National Structure Inventory records are for these features. \n",
    "\n",
    "These diagnostics also demonstrate that analysts may want to be careful in reporting expected annual losses alone, at least for single properties, since it can hide the bimodial and varying nature of expected annual loss estimates across a structure's ensemble members. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a595c3f7",
   "metadata": {},
   "source": [
    "A common use case is to plot property-level risk over space, and overlay social vulnerability data over that. Here, we will demonstrate how the reference and social vulnerability processing steps we implemented into our workflow make this easy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4feb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We processed CEJST and CDC SVI data\n",
    "# We will demonstrate the use case described above\n",
    "# with the CEJST data because it has an indicator variable\n",
    "# for when a community is considered \"disadvantaged\"\n",
    "# and we only return those census tracts,\n",
    "# which makes it a simpler plot\n",
    "cejst = gpd.read_file(join(VULN_DIR_I, 'social', FIPS, 'cejst.gpkg'))\n",
    "# So, we will also need census tract boundaries in our study area\n",
    "# to delineate areas that are not \"disadvantaged\"\n",
    "tract_geo = gpd.read_file(join(REF_DIR_I, FIPS, \"tract.gpkg\"))\n",
    "# We also wrote out a .gpkg file of the single family houses\n",
    "nsi_geo = gpd.read_file(join(EXP_DIR_I, FIPS, \"nsi_sf.gpkg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a0444f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can filter the single family houses to those which face\n",
    "# risk. We will first get the average expected annual loss\n",
    "# across ensemble members. We use the HAZUS DDFs here, but\n",
    "# you can test this out with the NACCS ones if you would like\n",
    "ens_agg = ens_df_f.groupby('fd_id')['hazus_eal'].mean().reset_index()\n",
    "\n",
    "nsi_risk = nsi_geo.merge(ens_agg,\n",
    "                         how='inner',\n",
    "                         on='fd_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f155d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Patch\n",
    "from matplotlib import ticker\n",
    "\n",
    "# Helps to zoom in on places where at-risk homes are\n",
    "# These were chosen to focus on an area where there\n",
    "# were many houses with high expected risk.\n",
    "minx = -75.10\n",
    "maxx = -75.065\n",
    "miny = 40\n",
    "maxy = 40.02\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4),\n",
    "                       dpi=300)\n",
    "\n",
    "# Reproject cejst to tract crs\n",
    "cejst_plot = cejst.to_crs(tract_geo.crs)\n",
    "\n",
    "# Plot the \"disadvantaged\" tracts\n",
    "# in light blue\n",
    "cejst_plot.plot(ax=ax,\n",
    "                color='#18437f',\n",
    "                alpha=.3,\n",
    "                edgecolor='none')\n",
    "\n",
    "# Plot census tract boundaries\n",
    "tract_geo.plot(ax=ax,\n",
    "               edgecolor='black',\n",
    "               color='none',\n",
    "               lw=.5)\n",
    "\n",
    "# Plot the eal\n",
    "plot_col = 'hazus_eal'\n",
    "vmin_eal = 0\n",
    "vmax_eal = nsi_risk[plot_col].max()\n",
    "# Sort before plotting\n",
    "nsi_plot = nsi_risk.sort_values(plot_col, ascending=True)\n",
    "nsi_plot.plot(ax=ax,\n",
    "              column=plot_col,\n",
    "              cmap='Reds',\n",
    "              s=1,\n",
    "              vmin=vmin_eal,\n",
    "              vmax=vmax_eal)\n",
    "\n",
    "# Set axis off but keep outline\n",
    "ax.tick_params(axis='both',\n",
    "               which='both',\n",
    "               bottom=False,\n",
    "               left=False,\n",
    "               labelbottom=False,\n",
    "               labelleft=False)\n",
    "\n",
    "# Set bounds\n",
    "ax.set_xlim([minx, maxx])\n",
    "ax.set_ylim([miny, maxy])\n",
    "\n",
    "legend_elements = [Patch(facecolor='#18437f',\n",
    "                         alpha=.3,\n",
    "                         label='Disadvantaged\\nCommunity'),]\n",
    "        \n",
    "ax.legend(handles=legend_elements,\n",
    "                  loc='center',\n",
    "                  fontsize='small',\n",
    "                  bbox_to_anchor=(.85, .19),\n",
    "                  frameon=False)\n",
    "\n",
    "# Add continuous legends\n",
    "cax = fig.add_axes([.21, 0.06, .61, 0.02])\n",
    "sm = plt.cm.ScalarMappable(cmap='Reds',\n",
    "                           norm=plt.Normalize(vmin=vmin_eal,\n",
    "                                              vmax=vmax_eal))\n",
    "sm._A = []\n",
    "cbr = fig.colorbar(sm, cax=cax, orientation='horizontal')\n",
    "cbr.ax.tick_params(labelsize=12, rotation=15) \n",
    "cbr.set_label(\"Average Risk Across Ensemble ($)\", size=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b2e726",
   "metadata": {},
   "source": [
    "Hopefully you have a sense for how unsafe standardizes risk _estimation_ under uncertainty so that you can focus on risk _assessment_ built on risk estimates. We will continue to add use-cases to our library of examples, and expand the functionality of unsafe for more occupancy types, damage functions, structure inventories, and more! Please consider contributing through development, testing, and requesting features. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsafe-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
