{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ad33027-9c79-4c04-8691-7784b32c92e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-31T13:41:57.085394Z",
     "iopub.status.busy": "2023-08-31T13:41:57.084855Z",
     "iopub.status.idle": "2023-08-31T13:41:57.093124Z",
     "shell.execute_reply": "2023-08-31T13:41:57.091231Z",
     "shell.execute_reply.started": "2023-08-31T13:41:57.085345Z"
    }
   },
   "source": [
    "# Configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8489d926-c191-4f5d-8b36-4980fe7fb469",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T13:57:01.145961Z",
     "iopub.status.busy": "2023-09-06T13:57:01.145433Z",
     "iopub.status.idle": "2023-09-06T13:57:01.186963Z",
     "shell.execute_reply": "2023-09-06T13:57:01.185470Z",
     "shell.execute_reply.started": "2023-09-06T13:57:01.145911Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef2ce263-d546-45ee-adc1-60147f24f389",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T13:57:01.496632Z",
     "iopub.status.busy": "2023-09-06T13:57:01.496123Z",
     "iopub.status.idle": "2023-09-06T13:57:03.971104Z",
     "shell.execute_reply": "2023-09-06T13:57:03.969656Z",
     "shell.execute_reply.started": "2023-09-06T13:57:01.496584Z"
    }
   },
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "import zipfile_deflate64\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import glob\n",
    "import shutil\n",
    "from os.path import join\n",
    "\n",
    "os.environ[\"USE_PYGEOS\"] = \"0\"\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "import rasterio.mask\n",
    "from pyproj import CRS\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76ea7c32-995b-4397-a2a1-668296d87e0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T13:57:03.972496Z",
     "iopub.status.busy": "2023-09-06T13:57:03.972225Z",
     "iopub.status.idle": "2023-09-06T13:57:03.990813Z",
     "shell.execute_reply": "2023-09-06T13:57:03.989821Z",
     "shell.execute_reply.started": "2023-09-06T13:57:03.972479Z"
    }
   },
   "outputs": [],
   "source": [
    "# Filepath directories\n",
    "\n",
    "# Get the absolute path to the project directory\n",
    "# Which is one directory above notebooks/\n",
    "ABS_DIR = os.path.abspath(Path(os.getcwd()).parents[0])\n",
    "# Get raw data directory\n",
    "FR = join(ABS_DIR, 'data', 'raw')\n",
    "# Get interim data directory\n",
    "FI = join(ABS_DIR, 'data', 'interim')\n",
    "\n",
    "# Directories for raw exposure, vulnerability (vuln) and \n",
    "# administrative reference files\n",
    "#  all exist so just need references\n",
    "EXP_DIR_R = join(FR, 'exposure')\n",
    "VULN_DIR_R = join(FR, 'vuln')\n",
    "REF_DIR_R = join(FR, 'ref')\n",
    "# Haz is for FEMA NFHL and depth grids\n",
    "HAZ_DIR_R = join(FR, 'haz')\n",
    "\n",
    "# Directories for interim exposure, vulnerability (vuln) and \n",
    "# hazard\n",
    "EXP_DIR_I = join(FI, 'exposure')\n",
    "VULN_DIR_I = join(FI, 'vuln')\n",
    "HAZ_DIR_I = join(FI, 'haz')\n",
    "REF_DIR_I = join(FI, 'ref')\n",
    "\n",
    "# Ensure they exist\n",
    "Path(EXP_DIR_I).mkdir(parents=True, exist_ok=True)\n",
    "Path(VULN_DIR_I).mkdir(parents=True, exist_ok=True)\n",
    "Path(HAZ_DIR_I).mkdir(parents=True, exist_ok=True)\n",
    "Path(REF_DIR_I).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Reference fips\n",
    "FIPS = '42101'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f546fa5e-7569-4116-9e69-c0a783490295",
   "metadata": {},
   "source": [
    "# Unzip Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6035f21a-49e8-4415-839d-ca5ceb0ccae5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-05T18:57:00.731903Z",
     "iopub.status.busy": "2023-09-05T18:57:00.731616Z",
     "iopub.status.idle": "2023-09-05T18:58:07.202098Z",
     "shell.execute_reply": "2023-09-05T18:58:07.200237Z",
     "shell.execute_reply.started": "2023-09-05T18:57:00.731882Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzipped and moved to interim : noaa\n",
      "Unzipped and moved to interim : tract\n",
      "Unzipped and moved to interim : block\n",
      "Unzipped and moved to interim : bg\n",
      "Unzipped and moved to interim : zcta\n",
      "Unzipped and moved to interim : county\n",
      "Unzipped and moved to interim : nfhl\n",
      "Unzipped and moved to interim : dg\n"
     ]
    }
   ],
   "source": [
    "# For each .zip directory in fr\n",
    "# Create needed subdirectories in interim/\n",
    "# Unzip in the appropriate interim/ subdirectory\n",
    "\n",
    "for path in Path(FR).rglob(\"*.zip\"):\n",
    "    # Avoid hidden files and files in directories\n",
    "    if path.name[0] != \".\":\n",
    "        # Get root for the directory this .zip file is in\n",
    "        zip_root = path.relative_to(FR).parents[0]\n",
    "\n",
    "        # Get path to interim/zip_root\n",
    "        zip_to_path = join(FI, zip_root)\n",
    "\n",
    "        # Make directory, including parents\n",
    "        # No need to check if directory exists bc\n",
    "        # it is only created when this script is run\n",
    "        Path(zip_to_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Unzip to zip_to_path\n",
    "        with ZipFile(path, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(zip_to_path)\n",
    "\n",
    "        print('Unzipped and moved to interim : '\n",
    "              + str(path.name).split('.')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14133e0-61f3-4035-a70a-4516a4d7670e",
   "metadata": {},
   "source": [
    "# Process NSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d9e8cfc3-cb2d-4925-9466-3ace1facc334",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-01T22:22:34.952391Z",
     "iopub.status.busy": "2023-09-01T22:22:34.951707Z",
     "iopub.status.idle": "2023-09-01T22:22:35.713707Z",
     "shell.execute_reply": "2023-09-01T22:22:35.712300Z",
     "shell.execute_reply.started": "2023-09-01T22:22:34.952339Z"
    }
   },
   "outputs": [],
   "source": [
    "# The NSI comes with all the data necessary for performing a standard \n",
    "# flood risk assessment. It is still useful to process the raw data.\n",
    "# Here, we subset to residential properties with 1 to 2 stories\n",
    "# and save as a geodataframe. These are the types of residences we have\n",
    "# multiple depth-damage functions for and a literature base to draw \n",
    "# from to introduce uncertainty in these loss estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c055a5ec-7b49-4e7a-891f-e1dad66c3b91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-05T19:37:01.373061Z",
     "iopub.status.busy": "2023-09-05T19:37:01.372477Z",
     "iopub.status.idle": "2023-09-05T19:37:04.719270Z",
     "shell.execute_reply": "2023-09-05T19:37:04.717943Z",
     "shell.execute_reply.started": "2023-09-05T19:37:01.373013Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read raw NSI data\n",
    "nsi_filep = join(EXP_DIR_R, 'nsi.pqt')\n",
    "# Read and reset index\n",
    "nsi_full = pd.read_parquet(nsi_filep).reset_index(drop=True)\n",
    "\n",
    "# Convert to geodataframe\n",
    "geometry = gpd.points_from_xy(nsi_full['properties.x'],\n",
    "                             nsi_full['properties.y'])\n",
    "# The NSI CRS is EPSG 4326\n",
    "nsi_gdf = gpd.GeoDataFrame(nsi_full, geometry=geometry,\n",
    "                           crs=\"EPSG:4326\")\n",
    "\n",
    "# Drop the following columns\n",
    "drop_cols = ['type', 'geometry.type', 'geometry.coordinates']\n",
    "nsi_gdf = nsi_gdf.drop(columns=drop_cols)\n",
    "\n",
    "# Remove \"properties\" from columns\n",
    "col_updates = [x.replace(\"properties.\", \"\") for x in nsi_gdf.columns]\n",
    "nsi_gdf.columns = col_updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2161a5f-2974-40d2-9962-ea5250559c52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-05T19:37:04.720092Z",
     "iopub.status.busy": "2023-09-05T19:37:04.719951Z",
     "iopub.status.idle": "2023-09-05T19:37:05.047248Z",
     "shell.execute_reply": "2023-09-05T19:37:05.046138Z",
     "shell.execute_reply.started": "2023-09-05T19:37:04.720079Z"
    }
   },
   "outputs": [],
   "source": [
    "# Subset to residential properties and update\n",
    "# RES 1 - single family\n",
    "# RES 2 - manufactured home\n",
    "# RES 3 - multifamily (but could fit into a depth-damage function\n",
    "# archetype depending on # stories)\n",
    "# We are going to use RES1 for this case-study\n",
    "# It is the only occtype with hazus and naccs\n",
    "# DDFs and has less ambiguous classification\n",
    "\n",
    "# occtype category for easier use in loss estimation steps\n",
    "\n",
    "# Get residential structures\n",
    "nsi_res = nsi_gdf.loc[nsi_gdf['occtype'].str[:4] == 'RES1']\n",
    "\n",
    "# For this case-study, don't use any building with more \n",
    "# than 2 stories\n",
    "res1_3s_ind = nsi_res['num_story'] > 2\n",
    "# Final residential dataframe\n",
    "res_f = nsi_res.loc[~res1_3s_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ac68d7e6-d767-44ee-a615-e8c315370a1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-05T15:25:38.117734Z",
     "iopub.status.busy": "2023-09-05T15:25:38.117545Z",
     "iopub.status.idle": "2023-09-05T15:26:40.409490Z",
     "shell.execute_reply": "2023-09-05T15:26:40.408246Z",
     "shell.execute_reply.started": "2023-09-05T15:25:38.117717Z"
    }
   },
   "outputs": [],
   "source": [
    "# Subset to relevant columns\n",
    "cols = ['fd_id', 'occtype', 'found_type', 'cbfips',\n",
    "        'ftprntsrc', 'found_ht', 'val_struct',\n",
    "        'val_cont', 'source', 'firmzone', 'ground_elv_m',\n",
    "        'geometry']\n",
    "\n",
    "res_out = res_f.loc[:,cols]\n",
    "\n",
    "# Write out to interim/exposure/\n",
    "EXP_OUT_FILEP = join(EXP_DIR_I, 'nsi_res.gpkg')\n",
    "res_out.to_file(EXP_OUT_FILEP, driver='GPKG')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2cb195-f0b5-4995-8321-4a168478c05b",
   "metadata": {},
   "source": [
    "# Prepare depth-damage functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23e58412-7c6d-4ffc-a7ff-81105e628708",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T13:57:09.802302Z",
     "iopub.status.busy": "2023-09-06T13:57:09.802087Z",
     "iopub.status.idle": "2023-09-06T13:57:10.599271Z",
     "shell.execute_reply": "2023-09-06T13:57:10.597480Z",
     "shell.execute_reply.started": "2023-09-06T13:57:09.802285Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read raw naccs data\n",
    "# vuln/physical is a directory w/ files that \n",
    "# is pre-supplied for the user of this codebase\n",
    "# The NACCS data is extracted from a pdf w/ manual entry\n",
    "# I entered it as a dataframe that mimics the way\n",
    "# Hazus enters data on DDFs so that it could potentially be \n",
    "# ingested into the Hazus database more easily\n",
    "naccs = pd.read_csv(join(VULN_DIR_R, 'physical', 'naccs_ddfs.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "aa5b64b5-3620-4fd2-b5a9-073fb9106457",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-01T22:23:52.797507Z",
     "iopub.status.busy": "2023-09-01T22:23:52.797030Z",
     "iopub.status.idle": "2023-09-01T22:23:52.875994Z",
     "shell.execute_reply": "2023-09-01T22:23:52.875160Z",
     "shell.execute_reply.started": "2023-09-01T22:23:52.797462Z"
    }
   },
   "outputs": [],
   "source": [
    "# Goal of processing is to have the data in\n",
    "# tidy format and with non string values\n",
    "\n",
    "# I think we should stick to RES1. Change\n",
    "# NSI processing above as well\n",
    "# Need to change occ type codes for pile \n",
    "# foundation\n",
    "\n",
    "# Drop Description and Source columns\n",
    "# Melt on occupancy damage category\n",
    "# Each depth is associated with a percent damage\n",
    "dropcols = ['Description', 'Source']\n",
    "idvars = ['Occupancy', 'DamageCategory']\n",
    "naccs_melt = naccs.drop(columns=dropcols).melt(id_vars=idvars,\n",
    "                                               var_name='depth_str',\n",
    "                                               value_name='pct_dam')\n",
    "\n",
    "# Need to convert depth_ft into a number\n",
    "# Replace ft with empty character\n",
    "# If string ends with m, make negative number\n",
    "# Else, make positive number\n",
    "naccs_melt['depth_str'] = naccs_melt['depth_str'].str.replace('ft', '')\n",
    "negdepth = naccs_melt.loc[naccs_melt['depth_str'].str[-1] == \n",
    "                          'm']['depth_str'].str[:-1].astype(float)*-1\n",
    "posdepth = naccs_melt.loc[naccs_melt['depth_str'].str[-1] != \n",
    "                          'm']['depth_str'].astype(float)\n",
    "\n",
    "naccs_melt.loc[naccs_melt['depth_str'].str[-1] == 'm',\n",
    "               'depth_ft'] = negdepth\n",
    "naccs_melt.loc[naccs_melt['depth_str'].str[-1] != 'm',\n",
    "               'depth_ft'] = posdepth\n",
    "\n",
    "# Divide pctdam by 100\n",
    "naccs_melt['rel_dam'] = naccs_melt['pct_dam']/100\n",
    "\n",
    "# Delete depth_str and pctdam and standardize\n",
    "# column names\n",
    "dropcols = ['depth_str', 'pct_dam']\n",
    "newcols = ['occtype', 'dam_cat', 'depth_ft', 'rel_dam']\n",
    "naccs_melt = naccs_melt.drop(columns=dropcols)\n",
    "naccs_melt.columns = newcols\n",
    "\n",
    "# Write out to processed/vulnerability/\n",
    "vuln_out_dir = join(VULN_DIR_I, 'physical')\n",
    "Path(vuln_out_dir).mkdir(parents=True, exist_ok=True)\n",
    "vuln_out_filep = join(vuln_out_dir, 'naccs_ddfs.csv')\n",
    "naccs_melt.to_csv(vuln_out_filep, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0063a40-6877-4690-8f29-fe533443ee7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T14:32:19.026459Z",
     "iopub.status.busy": "2023-09-06T14:32:19.025866Z",
     "iopub.status.idle": "2023-09-06T14:32:19.129624Z",
     "shell.execute_reply": "2023-09-06T14:32:19.128045Z",
     "shell.execute_reply.started": "2023-09-06T14:32:19.026407Z"
    }
   },
   "outputs": [],
   "source": [
    "# Got HAZUS DDFs from here: \n",
    "# https://github.com/cran/hazus/tree/master/data\n",
    "# Downloaded the Hazus 5.1 technical manual and created a \n",
    "# Riverine IDs spreadsheet that tracks what the current version of \n",
    "# HAZUS recommends for riverine DDFs. Will cross reference that with \n",
    "# the downloaded DDFs from the cran/hazus/data/ repository. \n",
    "# I loaded the data in R (itâ€™s .rda) and then converted to csv. \n",
    "\n",
    "# These are ddfs in the same form as the naccs data (I made the\n",
    "# naccs data conform to this as best as I could)\n",
    "hazus_ddfs = pd.read_csv(join(VULN_DIR_R, 'physical', 'haz_fl_dept.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df8fec62-1492-486a-860c-271c58830806",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T14:32:19.199965Z",
     "iopub.status.busy": "2023-09-06T14:32:19.199441Z",
     "iopub.status.idle": "2023-09-06T14:32:19.303535Z",
     "shell.execute_reply": "2023-09-06T14:32:19.302041Z",
     "shell.execute_reply.started": "2023-09-06T14:32:19.199917Z"
    }
   },
   "outputs": [],
   "source": [
    "# For basements, use FIA (MOD.) which does one and two floors by\n",
    "# A and V zones\n",
    "# For no basements, use USACE - IWR\n",
    "# which does one and two floor, no flood zone specified\n",
    "# 106: FIA (MOD.) 1S WB A zone\n",
    "# 114: \"\" V zone\n",
    "# 108: FIA (MOD.) 1S WB A zone\n",
    "# 116: \"\" V zone\n",
    "\n",
    "# 129: USACE - IWR 1S NB\n",
    "# 130: USCAE - IWR 2S+ NB\n",
    "\n",
    "# Handling pile and pier foundations is important\n",
    "# for RES1, but this will not be an issue for this case-study\n",
    "# since there are no foundation types like this in the NSI\n",
    "# for Philly (is that true, though?)\n",
    "\n",
    "# Subset to DmgFnId in the codes above\n",
    "dmg_ids = [106, 108, 114, 116, 129, 130]\n",
    "hazus_res = hazus_ddfs[(hazus_ddfs['DmgFnId'].isin(dmg_ids)) &\n",
    "                       (hazus_ddfs['Occupancy'] == 'RES1')]\n",
    "\n",
    "# Make occtype column in the same form that the NSI has\n",
    "# e.g. RES1-1SNB\n",
    "# Add column for A or V zone\n",
    "# Note: outside SFHA basement homes will take A zone\n",
    "# What other option do we have? \n",
    "\n",
    "# Split Description by comma. \n",
    "# The split[0] element tells us stories (but description sometimes\n",
    "# says floors instead of story...)\n",
    "# Can get around this issue by looking at first word\n",
    "# The split[1] element\n",
    "# tells us w/ basement or no basement. Use this to create occtype\n",
    "desc = hazus_res['Description'].str.split(',')\n",
    "s_type = desc.str[0].str.split(' ').str[0]\n",
    "s_type = s_type.str.replace('one', '1').str.replace('two', '2')\n",
    "b_type = desc.str[1].str.strip()\n",
    "occtype = np.where(b_type == 'w/ basement',\n",
    "                   s_type + 'SWB',\n",
    "                   s_type + 'SNB')\n",
    "fz = desc.str[-1].str.replace('Structure', '').str.strip()\n",
    "\n",
    "# Need occtype, flood zone, depth_ft, and rel_dam columns\n",
    "# Follow steps from naccs processing to get depth_ft and rel_dam\n",
    "# First, drop unecessary columns\n",
    "# Don't need Source_Table, Occupy_Class, Cover_Class, empty columns\n",
    "# Description, Source, DmgFnId, Occupancy and first col (Unnamed: 0)\n",
    "# because index was written out\n",
    "# Don't need all na columns either (just for automobiles, apparently)\n",
    "hazus_res = hazus_res.loc[:,[col for col in hazus_res.columns if 'ft' in col]]\n",
    "hazus_res = hazus_res.dropna(axis=1, how='all')\n",
    "# Add the occtype and fld_zone columns\n",
    "hazus_res = hazus_res.assign(occtype=occtype,\n",
    "                             fld_zone=fz.str[0])\n",
    "\n",
    "# Then, occtype and fld_zone as index and melt rest of columns. Following \n",
    "# naccs processing\n",
    "idvars = ['occtype', 'fld_zone']\n",
    "hazus_melt = hazus_res.melt(id_vars=idvars,\n",
    "                            var_name='depth_str',\n",
    "                            value_name='pct_dam')\n",
    "\n",
    "# Need to convert depth_ft into a number\n",
    "# Replace ft with empty character\n",
    "# If string ends with m, make negative number\n",
    "# Else, make positive number\n",
    "hazus_melt['depth_str'] = hazus_melt['depth_str'].str.replace('ft', '')\n",
    "negdepth = hazus_melt.loc[hazus_melt['depth_str'].str[-1] == \n",
    "                          'm']['depth_str'].str[:-1].astype(float)*-1\n",
    "posdepth = hazus_melt.loc[hazus_melt['depth_str'].str[-1] != \n",
    "                          'm']['depth_str'].astype(float)\n",
    "\n",
    "hazus_melt.loc[hazus_melt['depth_str'].str[-1] == 'm',\n",
    "               'depth_ft'] = negdepth\n",
    "hazus_melt.loc[hazus_melt['depth_str'].str[-1] != 'm',\n",
    "               'depth_ft'] = posdepth\n",
    "\n",
    "# Divide pctdam by 100\n",
    "hazus_melt['rel_dam'] = hazus_melt['pct_dam']/100\n",
    "\n",
    "# Delete depth_str and pctdam and standardize\n",
    "# column names\n",
    "# Since we just have the building types, call this\n",
    "# bld_type instead of occtype\n",
    "dropcols = ['depth_str', 'pct_dam']\n",
    "newcols = ['bld_type', 'fld_zone', 'depth_ft', 'rel_dam']\n",
    "hazus_melt = hazus_melt.drop(columns=dropcols)\n",
    "hazus_melt.columns = newcols\n",
    "\n",
    "# Write out to processed/vulnerability/\n",
    "vuln_out_dir = join(VULN_DIR_I, 'physical')\n",
    "Path(vuln_out_dir).mkdir(parents=True, exist_ok=True)\n",
    "vuln_out_filep = join(vuln_out_dir, 'hazus_ddfs.csv')\n",
    "hazus_melt.to_csv(vuln_out_filep, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b915f6-da76-4f7a-8d42-e37f4c13b1dc",
   "metadata": {},
   "source": [
    "# Process Hazard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "15901b73-fb6d-4c37-9fef-fbf941b1f9ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-05T15:46:53.668121Z",
     "iopub.status.busy": "2023-09-05T15:46:53.667558Z",
     "iopub.status.idle": "2023-09-05T15:46:55.736588Z",
     "shell.execute_reply": "2023-09-05T15:46:55.735468Z",
     "shell.execute_reply.started": "2023-09-05T15:46:53.668071Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save the flood zones, do some processing on columns\n",
    "# The files are unzipped as shape files instead of gdb\n",
    "# We want S_FLD_HAZ_AR \n",
    "fld_haz_fp = join(HAZ_DIR_I, 'nfhl', 'S_FLD_HAZ_AR.shp')\n",
    "nfhl = gpd.read_file(fld_haz_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "06340099-28e7-49ef-a999-6b44c48925ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-05T15:49:36.876320Z",
     "iopub.status.busy": "2023-09-05T15:49:36.875763Z",
     "iopub.status.idle": "2023-09-05T15:49:38.389726Z",
     "shell.execute_reply": "2023-09-05T15:49:38.388188Z",
     "shell.execute_reply.started": "2023-09-05T15:49:36.876271Z"
    }
   },
   "outputs": [],
   "source": [
    "# Keep FLD_ZONE, FLD_AR_ID, STATIC_BFE, geometry\n",
    "keep_cols = ['FLD_ZONE', 'FLD_AR_ID', 'STATIC_BFE', 'ZONE_SUBTY',\n",
    "             'geometry']\n",
    "nfhl_f = nfhl.loc[:,keep_cols]\n",
    "\n",
    "# Adjust .2 pct X zones to X_500\n",
    "nfhl_f.loc[nfhl_f['ZONE_SUBTY'] == '0.2 PCT ANNUAL CHANCE FLOOD HAZARD',\n",
    "           'FLD_ZONE'] = nfhl_f['FLD_ZONE'] + '_500'\n",
    "\n",
    "# Update column names\n",
    "# Lower case\n",
    "nfhl_f.columns = [x.lower() for x in nfhl_f.columns]\n",
    "\n",
    "# Drop ZONE_SUBTY\n",
    "nfhl_f = nfhl_f.drop(columns=['zone_subty'])\n",
    "\n",
    "# Write file\n",
    "nfhl_f.to_file(join(HAZ_DIR_I, 'fld_zones.gpkg'),\n",
    "               driver='GPKG')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "74c21443-b419-445b-acb0-03a70a8beb04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-05T15:49:59.629404Z",
     "iopub.status.busy": "2023-09-05T15:49:59.628836Z",
     "iopub.status.idle": "2023-09-05T15:50:00.350637Z",
     "shell.execute_reply": "2023-09-05T15:50:00.349028Z",
     "shell.execute_reply.started": "2023-09-05T15:49:59.629354Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is optional: delete the nfhl directory to reduce\n",
    "# the file storage burden\n",
    "# TODO: Make this a setting in a config file you can toggle as a user\n",
    "RM_NFHL = True\n",
    "if RM_NFHL:\n",
    "    # Get directory name\n",
    "    nfhl_dir = join(HAZ_DIR_I, 'nfhl')\n",
    "    \n",
    "    # Try to remove the tree; if it fails,\n",
    "    # throw an error using try...except.\n",
    "    try:\n",
    "        shutil.rmtree(nfhl_dir)\n",
    "    except OSError as e:\n",
    "        print(\"Error: %s - %s.\" % (e.filename, e.strerror))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6879df0-a805-4e9e-9ef3-9b61ba40a430",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-05T19:03:13.511933Z",
     "iopub.status.busy": "2023-09-05T19:03:13.511024Z",
     "iopub.status.idle": "2023-09-05T19:03:13.642159Z",
     "shell.execute_reply": "2023-09-05T19:03:13.641001Z",
     "shell.execute_reply.started": "2023-09-05T19:03:13.511891Z"
    }
   },
   "outputs": [],
   "source": [
    "# No need to do anything for the depth rasters. Can \n",
    "# remove some extemperaneous files to reduce \n",
    "# size of project\n",
    "# This is optional processing - make that clear\n",
    "# In the interim/haz/dg directory, \n",
    "# only keep files that start with Depth_\n",
    "# Or CstDpth_\n",
    "# Can loop through files in the directory and keep all files\n",
    "# that start with these characters\n",
    "dg_dir = join(HAZ_DIR_I, 'dg')\n",
    "for path in Path(dg_dir).iterdir():\n",
    "    if (str(path.name[:5]) != 'Depth') and (str(path.name[:7]) != 'CstDpth'):\n",
    "        path.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37f7402-1c85-4d92-8f64-b75ae89fadfb",
   "metadata": {},
   "source": [
    "# Process Reference Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "88399cad-21d1-4109-a1fb-dd3367006172",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-01T22:24:00.239713Z",
     "iopub.status.busy": "2023-09-01T22:24:00.239539Z",
     "iopub.status.idle": "2023-09-01T22:24:00.257631Z",
     "shell.execute_reply": "2023-09-01T22:24:00.256581Z",
     "shell.execute_reply.started": "2023-09-01T22:24:00.239697Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get all the reference files oriented to the county\n",
    "# Save as .gpkg\n",
    "# Clear ref/ directory when done\n",
    "\n",
    "# base file name for state files\n",
    "base_state_fp = 'tl_2022_42_'\n",
    "\n",
    "# base file name for us files\n",
    "base_us_fp = 'tl_2022_us_'\n",
    "\n",
    "# state based files\n",
    "state_ref_l = ['bg.shp', 'tabblock20.shp', 'tract.shp']\n",
    "state_ref_l = [base_state_fp + x for x in state_ref_l]\n",
    "\n",
    "# us based files\n",
    "us_ref_l = ['zcta520.shp']\n",
    "us_ref_l = [base_us_fp + x for x in us_ref_l]\n",
    "\n",
    "# merge list\n",
    "ref_l = state_ref_l + us_ref_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e452348b-1c76-43de-a076-bc5a5b0c8ee5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-01T22:24:00.258574Z",
     "iopub.status.busy": "2023-09-01T22:24:00.258407Z",
     "iopub.status.idle": "2023-09-01T22:26:12.829216Z",
     "shell.execute_reply": "2023-09-01T22:26:12.828492Z",
     "shell.execute_reply.started": "2023-09-01T22:24:00.258559Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clipped reference data: bg\n",
      "Clipped reference data: tabblock20\n",
      "Clipped reference data: tract\n",
      "Clipped reference data: zcta520\n"
     ]
    }
   ],
   "source": [
    "# Read in county file\n",
    "counties = gpd.read_file(join(REF_DIR_I, base_us_fp + 'county.shp'))\n",
    "\n",
    "# Identify county from geoid column\n",
    "# If processing multiple counties in the future, \n",
    "# can change to .isin(FIPS) w/ FIPS as a list\n",
    "# Or, more generally can write a wrapper function\n",
    "# for handling string or list input\n",
    "counties_f = counties.loc[counties['GEOID'] == FIPS][['geometry']]\n",
    "counties_f['fips'] = FIPS\n",
    "\n",
    "# Use as reference for clipping other files\n",
    "# For each polygon in counties_f, which corresponds to a county,\n",
    "# you want to check temp_ref.within(polygon) and add these ref \n",
    "# polygons to a dataframe \n",
    "\n",
    "\n",
    "# Need a dict for these\n",
    "# Start with counties_f which we need to write out\n",
    "ref_clip_l = {'counties': counties_f}\n",
    "\n",
    "# Loop through other data, clip to county, save as .gpkg\n",
    "for ref in ref_l:\n",
    "    # Read in the ref file\n",
    "    temp_ref = gpd.read_file(join(REF_DIR_I, ref))\n",
    "    # Do a spatial join for ref in county(ies)\n",
    "    temp_ref_j = gpd.sjoin(temp_ref, counties_f, predicate='within')\n",
    "    # Add to ref clip list\n",
    "    # key/value pairs are refname.gpkg which can be obtained\n",
    "    # by splitting refname.shp on '.' and keeping first part of string\n",
    "    # then getting the last name after splitting on '_'\n",
    "    ref_name = ref.split('.')[0].split('_')[-1]\n",
    "    ref_clip_l[ref_name] = temp_ref_j\n",
    "    # Helpful log message\n",
    "    print('Clipped reference data: ' + ref_name)\n",
    "\n",
    "# Delete the shp files in interim/ref\n",
    "for path in Path(REF_DIR_I).iterdir():\n",
    "    path.unlink()\n",
    "\n",
    "# Save the gpkg files for each ref\n",
    "# I could probably write out the gpkg into \n",
    "# a different directory, or something else\n",
    "# that makes it obvious to just write the\n",
    "# file out when you loop through ref_l above\n",
    "# But reference files don't seem like\n",
    "# processed data -- interim seems right\n",
    "# An improvement might be having a temp directory\n",
    "# for storing unzipped files and then\n",
    "# it makes sense to clean these up\n",
    "# after processing and moving certain files\n",
    "# to interim. That seems neater and more\n",
    "# nominally correct\n",
    "for ref_name, ref in ref_clip_l.items():\n",
    "    ref_out_fp = join(REF_DIR_I, ref_name + '.gpkg')\n",
    "    # Wrote out ref data as gpkg\n",
    "    ref.to_file(ref_out_fp, driver='GPKG')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb2ac05-9ece-41d0-8d52-082d0d247bdb",
   "metadata": {},
   "source": [
    "# Process social vulnerability data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b037850-5b5c-404a-8e61-4099265617f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-05T14:06:14.950357Z",
     "iopub.status.busy": "2023-09-05T14:06:14.950119Z",
     "iopub.status.idle": "2023-09-05T14:06:20.039520Z",
     "shell.execute_reply": "2023-09-05T14:06:20.038744Z",
     "shell.execute_reply.started": "2023-09-05T14:06:14.950338Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read NOAA SOVI\n",
    "vuln_s_path = join(VULN_DIR_I, 'social')\n",
    "sovi_filep = join(vuln_s_path, 'SoVI2010_PA')\n",
    "sovi = gpd.read_file(sovi_filep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ce63a1d-8b5e-4462-9715-1c805c528b0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-05T14:06:26.259786Z",
     "iopub.status.busy": "2023-09-05T14:06:26.259568Z",
     "iopub.status.idle": "2023-09-05T14:06:26.290306Z",
     "shell.execute_reply": "2023-09-05T14:06:26.289057Z",
     "shell.execute_reply.started": "2023-09-05T14:06:26.259768Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get fips col\n",
    "fips = sovi['STATEFP10'] + sovi['COUNTYFP10']\n",
    "sovi = sovi.assign(fips=fips)\n",
    "sovi_fips = sovi[sovi['fips'].eq(FIPS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3749f38b-7470-4206-a921-72f9a8889b8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-05T14:06:26.949692Z",
     "iopub.status.busy": "2023-09-05T14:06:26.948902Z",
     "iopub.status.idle": "2023-09-05T14:06:27.046532Z",
     "shell.execute_reply": "2023-09-05T14:06:27.045861Z",
     "shell.execute_reply.started": "2023-09-05T14:06:26.949645Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load in county reference file\n",
    "fips_geo_fp = join(REF_DIR_I, 'counties.gpkg')\n",
    "fips_geo = gpd.read_file(fips_geo_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b0678ed3-e08e-41a4-b093-fe4d13917d10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-05T14:06:27.550802Z",
     "iopub.status.busy": "2023-09-05T14:06:27.549644Z",
     "iopub.status.idle": "2023-09-05T14:06:27.583305Z",
     "shell.execute_reply": "2023-09-05T14:06:27.582074Z",
     "shell.execute_reply.started": "2023-09-05T14:06:27.550746Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visual check that boundaries of SOVI are snapped\n",
    "# to the county boundary correctly - looks good\n",
    "# fig, ax = plt.subplots(figsize=(10, 6))\n",
    "# sovi_plot = sovi_fips.to_crs(fips_geo.crs)\n",
    "# sovi_plot.plot(ax=ax, color='blue', alpha=.2)\n",
    "# fips_geo.plot(ax=ax, color='red', alpha=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "484720f5-8090-4ae3-8106-42e71646c10f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-05T14:09:27.537673Z",
     "iopub.status.busy": "2023-09-05T14:09:27.537364Z",
     "iopub.status.idle": "2023-09-05T14:09:28.359633Z",
     "shell.execute_reply": "2023-09-05T14:09:28.358057Z",
     "shell.execute_reply.started": "2023-09-05T14:09:27.537648Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reproject sovi_fips to county crs\n",
    "sovi_f = sovi_fips.to_crs(fips_geo.crs)\n",
    "\n",
    "# Add column for \"high\" vulnerability based on\n",
    "# FMA grant designation of .75 (greater than)\n",
    "# Use the column 'SOVI0610PA' for the score\n",
    "sovi_f['FMA_High'] = False\n",
    "sovi_f.loc[sovi_f['SOVI0610PA'] > .75, 'FMA_High'] = True\n",
    "\n",
    "# Keep relevant columns\n",
    "keep_cols = ['fips', 'SOVI0610PA', 'SoVI0610_1', 'SoVI0610_2',\n",
    "             'GEOID10', 'geometry']\n",
    "sovi_f = sovi_f.loc[:, keep_cols]\n",
    "\n",
    "# Rename columns\n",
    "rename_cols = ['fips', 'sovi', 'sovi_cat_1', 'sovi_cat_2',\n",
    "               'tract_id', 'geometry']\n",
    "sovi_f.columns = rename_cols\n",
    "\n",
    "# Delete the unzipped files dir to remove file junk\n",
    "sovi_dir = join(vuln_s_path, 'SoVI2010_PA')\n",
    "# This is optional: delete the sovi directory to reduce\n",
    "# the file storage burden\n",
    "# TODO: Make this a setting in a config file you can toggle as a user\n",
    "RM_SOVI = True\n",
    "if RM_SOVI:\n",
    "    # Try to remove the tree; if it fails,\n",
    "    # throw an error using try...except.\n",
    "    try:\n",
    "        shutil.rmtree(sovi_dir)\n",
    "    except OSError as e:\n",
    "        print(\"Error: %s - %s.\" % (e.filename, e.strerror))\n",
    "\n",
    "# Write out the new SOVI in interim\n",
    "sovi_out_path = join(vuln_s_path, 'sovi.gpkg')\n",
    "sovi_f.to_file(sovi_out_path, driver='GPKG')\n",
    "    \n",
    "# (It really makes sense to have a temp/ directory for unzipped\n",
    "# stuff so we can just delete it all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da59a5a3-52c5-456c-89a7-d45a90245ae8",
   "metadata": {},
   "source": [
    "# Link NSI with Hazard Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af0a8272-e19e-41ed-abf2-3db6a804609d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-05T19:12:32.090977Z",
     "iopub.status.busy": "2023-09-05T19:12:32.090420Z",
     "iopub.status.idle": "2023-09-05T19:13:18.352631Z",
     "shell.execute_reply": "2023-09-05T19:13:18.351829Z",
     "shell.execute_reply.started": "2023-09-05T19:12:32.090930Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load NSI\n",
    "nsi = gpd.read_file(join(EXP_DIR_I, 'nsi_res.gpkg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b9ae7d-1439-4f32-94b3-06e94902ae45",
   "metadata": {},
   "source": [
    "## Structures & Flood Zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c343db81-34f3-4c25-bccd-5d7fb04ec53d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-05T15:52:26.202086Z",
     "iopub.status.busy": "2023-09-05T15:52:26.201521Z",
     "iopub.status.idle": "2023-09-05T15:52:29.098748Z",
     "shell.execute_reply": "2023-09-05T15:52:29.097633Z",
     "shell.execute_reply.started": "2023-09-05T15:52:26.202037Z"
    }
   },
   "outputs": [],
   "source": [
    "# NSI has firmzone column but it was all NaN when I pulled it\n",
    "# from the API. We need to get flood zones to know which properties\n",
    "# are in or outside the SFHA. This is useful for summary stats. \n",
    "# Down the line, we will also want to know things like pre/post FIRM\n",
    "# for adjusting FFE relative to BFE, probabilities of foundation\n",
    "# types, etc. \n",
    "\n",
    "# Read in flood zones\n",
    "nfhl_filep = join(HAZ_DIR_I, 'fld_zones.gpkg')\n",
    "fz = gpd.read_file(nfhl_filep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7de64c5d-1064-4643-b145-d46a228c03a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-05T15:53:30.987710Z",
     "iopub.status.busy": "2023-09-05T15:53:30.987470Z",
     "iopub.status.idle": "2023-09-05T15:53:34.390314Z",
     "shell.execute_reply": "2023-09-05T15:53:34.389049Z",
     "shell.execute_reply.started": "2023-09-05T15:53:30.987693Z"
    }
   },
   "outputs": [],
   "source": [
    "# Project nsi to flood zone crs\n",
    "nsi_rep = nsi.to_crs(fz.crs)\n",
    "\n",
    "# Spatial join, retaining flood zone cols\n",
    "# Only need the id and geom from nsi for this\n",
    "fz_m = gpd.sjoin(nsi_rep[['fd_id', 'geometry']],\n",
    "                 fz,\n",
    "                 predicate='within')\n",
    "\n",
    "# I checked for issues like overlapping flood zones\n",
    "# resulting in NSI structures in multiple polygons\n",
    "# and did not find any. That's good, but chances\n",
    "# are there will be counties where this happens\n",
    "# and we will need code to handle these consistently\n",
    "\n",
    "# Write out fd_id/fld_ar_id/fld_zone/static_bfe\n",
    "keep_cols = ['fd_id', 'fld_zone', 'fld_ar_id', 'static_bfe']\n",
    "fz_m_out = fz_m[keep_cols]\n",
    "\n",
    "nsi_fz_filep = join(EXP_DIR_I, 'nsi_fz.pqt')\n",
    "fz_m_out.to_parquet(nsi_fz_filep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dda31ba-89cd-4d2e-b623-68f3cf82b7a1",
   "metadata": {},
   "source": [
    "## Structures and Depth Grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10b85ff1-3cc5-4909-8080-41faf48be7b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-05T19:11:27.612918Z",
     "iopub.status.busy": "2023-09-05T19:11:27.612348Z",
     "iopub.status.idle": "2023-09-05T19:11:28.777402Z",
     "shell.execute_reply": "2023-09-05T19:11:28.776127Z",
     "shell.execute_reply.started": "2023-09-05T19:11:27.612868Z"
    }
   },
   "outputs": [],
   "source": [
    "# For each depth grid, we need to link up the depth with the structure\n",
    "# Reproject structures to the depth grid crs\n",
    "# Do point in grid calculations\n",
    "# Write out a file in interim for the fd_id/depth dataframe\n",
    "dg_dir = join(HAZ_DIR_I, 'dg')\n",
    "# We are going to store each depth grid\n",
    "# in a dict with the return period as the key\n",
    "dg_dict = {}\n",
    "for path in Path(dg_dir).rglob(\"*.tif\"):\n",
    "    # filename will be of the form\n",
    "    # Depth_RPpct.tif\n",
    "    # Or CstDpth_RPpct.tif\n",
    "    filename = path.name\n",
    "\n",
    "    # Unfortunately, the file naming conventions for\n",
    "    # the inland and coastal depth grids are different\n",
    "    # Not a huge deal, but makes the code look less nice\n",
    "\n",
    "    # Figure out if Inl or Cst\n",
    "    fld_src = 'Inl' if str(filename[:5]) == 'Depth' else 'Cst'\n",
    "\n",
    "    if fld_src == 'Inl':\n",
    "        # Below is how you process the inland depth grids\n",
    "        # This drops \"Depth\" and combines remaining split\n",
    "        # string pieces with a '.'\n",
    "        # This is designed specifically for the way the fema\n",
    "        # dgs are stored. It has the nice feature of giving us\n",
    "        # 0.2 instead of 0_2 for the .2% prob event\n",
    "        # Then we drop pct.tif, the last 7 characters\n",
    "        ret_p_temp = '.'.join(filename.split('_')[1:])\n",
    "        ret_p = ret_p_temp[:-7]\n",
    "    else:\n",
    "        # This will get you 0_2 to 0.2 and otherwise return 01, 02, 10\n",
    "        ret_p = filename.split('pct')[0][7:].replace('_', '.')\n",
    "    \n",
    "    # Let's load each file and store in a dictionary that\n",
    "    # uses the fld_src + return period as the key\n",
    "    dg_dict[fld_src + '_' + ret_p] = rasterio.open(path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "950aa3bf-4b2e-43e1-8899-3c65b7925c1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-05T19:13:18.353847Z",
     "iopub.status.busy": "2023-09-05T19:13:18.353697Z",
     "iopub.status.idle": "2023-09-05T19:13:19.303180Z",
     "shell.execute_reply": "2023-09-05T19:13:19.302469Z",
     "shell.execute_reply.started": "2023-09-05T19:13:18.353834Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get CRS from the first depth grid\n",
    "# Reproject nsi for this CRS\n",
    "dg_temp = list(dg_dict.values())[0] \n",
    "dg_crs = dg_temp.crs\n",
    "\n",
    "nsi_reproj = nsi.to_crs(dg_crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ded3a6d-3d0d-405f-9737-eec2c79e9c6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-05T19:13:19.303825Z",
     "iopub.status.busy": "2023-09-05T19:13:19.303708Z",
     "iopub.status.idle": "2023-09-05T19:15:13.947229Z",
     "shell.execute_reply": "2023-09-05T19:15:13.946491Z",
     "shell.execute_reply.started": "2023-09-05T19:13:19.303813Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Store NSI coordinates in list\n",
      "Flooding source: Inl\n",
      "Return period: 0.2\n",
      "Sampled depth from grid:\n",
      "Aligned depths with index of structures in series\n",
      "Flooding source: Cst\n",
      "Return period: 10\n",
      "Sampled depth from grid:\n",
      "Aligned depths with index of structures in series\n",
      "Flooding source: Cst\n",
      "Return period: 0.2\n",
      "Sampled depth from grid:\n",
      "Aligned depths with index of structures in series\n",
      "Flooding source: Cst\n",
      "Return period: 02\n",
      "Sampled depth from grid:\n",
      "Aligned depths with index of structures in series\n",
      "Flooding source: Inl\n",
      "Return period: 01\n",
      "Sampled depth from grid:\n",
      "Aligned depths with index of structures in series\n",
      "Flooding source: Inl\n",
      "Return period: 02\n",
      "Sampled depth from grid:\n",
      "Aligned depths with index of structures in series\n",
      "Flooding source: Cst\n",
      "Return period: 01\n",
      "Sampled depth from grid:\n",
      "Aligned depths with index of structures in series\n",
      "Flooding source: Inl\n",
      "Return period: 10\n",
      "Sampled depth from grid:\n",
      "Aligned depths with index of structures in series\n"
     ]
    }
   ],
   "source": [
    "# For each depth grid, we will sample from the grid\n",
    "# by way of a list of coordinates from the reprojected\n",
    "# nsi geodataframe (this is the fastest way I know to do it)\n",
    "coords = zip(nsi_reproj['geometry'].x, nsi_reproj['geometry'].y)\n",
    "coord_list = [(x, y) for x, y in coords]\n",
    "print('Store NSI coordinates in list')\n",
    "\n",
    "# We'll store series of fd_id/depth pairs for each return period\n",
    "# in a list and concat this into a df after iterating\n",
    "depth_list = []\n",
    "for key, dg in dg_dict.items():\n",
    "    fld_src = key.split('_')[0]\n",
    "    rp = key.split('_')[1]\n",
    "    print('Flooding source: ' + fld_src)\n",
    "    print('Return period: ' + rp)\n",
    "    # Sample from the depth grid based on structure locations\n",
    "    # I did some ground truthing in qgis\n",
    "    # It appears that the sampled values align correctly\n",
    "    sampled_depths = [x[0] for x in dg.sample(coord_list)]\n",
    "    print('Sampled depth from grid:')\n",
    "\n",
    "    # Store the series \n",
    "    depths = pd.Series(sampled_depths,\n",
    "                       index=nsi_reproj['fd_id'],\n",
    "                       name=key)\n",
    "    # Add the series to the list of series\n",
    "    depth_list.append(depths)\n",
    "    print('Aligned depths with index of structures in series')\n",
    "\n",
    "# Concat to dataframe\n",
    "depth_df = pd.concat(depth_list, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cfe2432e-3b1f-44e6-bb28-699a5f5adbae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-05T19:16:00.534245Z",
     "iopub.status.busy": "2023-09-05T19:16:00.533998Z",
     "iopub.status.idle": "2023-09-05T19:16:01.497965Z",
     "shell.execute_reply": "2023-09-05T19:16:01.496427Z",
     "shell.execute_reply.started": "2023-09-05T19:16:00.534228Z"
    }
   },
   "outputs": [],
   "source": [
    "# Replace nodata values with 0\n",
    "depth_df[depth_df == dg_temp.nodata] = 0\n",
    "\n",
    "# Get a dataframe of structures with flood exposure\n",
    "depth_df_f = depth_df[depth_df.sum(axis=1) > 0]\n",
    "\n",
    "# Multiply by 3.281 to convert to feet\n",
    "depth_df_f = depth_df_f*3.281"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e670fe5-7982-4388-982a-612e9f6fc3c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-05T19:16:03.401667Z",
     "iopub.status.busy": "2023-09-05T19:16:03.401298Z",
     "iopub.status.idle": "2023-09-05T19:16:03.690646Z",
     "shell.execute_reply": "2023-09-05T19:16:03.689903Z",
     "shell.execute_reply.started": "2023-09-05T19:16:03.401635Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write out file of fd_id and depths\n",
    "# Reset index to get fd_id as a column\n",
    "# (I don't think it really matters but to be consistent\n",
    "# with earlier file writing decisions)\n",
    "# Flood depths are relative to grade\n",
    "nsi_depths_filep = join(EXP_DIR_I, 'nsi_depths.pqt')\n",
    "depth_df_f.reset_index().to_parquet(nsi_depths_filep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5249f4d5-6472-4a37-b860-1203417ad2ca",
   "metadata": {},
   "source": [
    "# Link NSI with reference data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ab62f88f-d465-4562-990a-d79c6fb70379",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-05T19:48:19.491902Z",
     "iopub.status.busy": "2023-09-05T19:48:19.491724Z",
     "iopub.status.idle": "2023-09-05T19:49:04.061461Z",
     "shell.execute_reply": "2023-09-05T19:49:04.060725Z",
     "shell.execute_reply.started": "2023-09-05T19:48:19.491888Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load NSI\n",
    "nsi = gpd.read_file(join(EXP_DIR_I, 'nsi_res.gpkg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7dc2e09e-673a-4d09-ab49-17938765b2c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-05T19:49:04.062395Z",
     "iopub.status.busy": "2023-09-05T19:49:04.062265Z",
     "iopub.status.idle": "2023-09-05T19:49:17.647812Z",
     "shell.execute_reply": "2023-09-05T19:49:17.647086Z",
     "shell.execute_reply.started": "2023-09-05T19:49:04.062384Z"
    }
   },
   "outputs": [],
   "source": [
    "# Want to know which block, block group, zip, tract structures\n",
    "# are in. Loop through these, do spatial joins, and\n",
    "# write the nsi_ref file to exposure\n",
    "\n",
    "# These could all be defined in config files\n",
    "refs = ['tabblock20', 'bg',  'tract', 'zcta520']\n",
    "ref_id = ['GEOID20', 'GEOID', 'GEOID', 'GEOID20']\n",
    "ref_names = ['block', 'bg', 'tract', 'zcta']\n",
    "ref_df_list = []\n",
    "# The logic here is to read in the ref file\n",
    "# Subset just to the id column \n",
    "# Reproject NSI to the crs of the ref file\n",
    "# Do a spatial join for point in polygon\n",
    "# Then store an indexed dataframe of the ref id so that\n",
    "# we can concat all of them together later on fd_id\n",
    "for i, ref in enumerate(refs):\n",
    "    ref_filep = join(REF_DIR_I, ref + '.gpkg')\n",
    "    ref_geo = gpd.read_file(ref_filep)[[ref_id[i], 'geometry']]\n",
    "    nsi_reproj = nsi.to_crs(ref_geo.crs)[['fd_id', 'geometry']]\n",
    "    nsi_ref = gpd.sjoin(nsi_reproj, ref_geo, predicate='within')\n",
    "    nsi_ref_f = nsi_ref.set_index('fd_id')[[ref_id[i]]]\n",
    "    nsi_ref_f = nsi_ref_f.rename(columns={ref_id[i]: ref_names[i] + '_id'})\n",
    "    ref_df_list.append(nsi_ref_f)\n",
    "\n",
    "nsi_refs = pd.concat(ref_df_list, axis=1).reset_index()\n",
    "# Write to file - in exposure\n",
    "ref_filep = join(EXP_DIR_I, 'nsi_ref.pqt')\n",
    "nsi_refs.to_parquet(ref_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeb4b9c-e5bf-435a-8f1b-83ca7d25b46d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flrisk",
   "language": "python",
   "name": "flrisk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
